"""
ä½¿ç”¨ LLM æ„å»º UserProfile çš„ç¤ºä¾‹

æœ¬ç¤ºä¾‹æ¼”ç¤ºå¦‚ä½•ï¼š
1. ä½¿ç”¨ app/services/llm_adapter è°ƒç”¨ LLMï¼ˆæ”¯æŒæŒ‡å®šå¹³å°ï¼‰
2. åŸºäºå¯¹è¯å†å²åˆ†æç”¨æˆ·ç”»åƒå’Œåœºæ™¯
3. ä½¿ç”¨ core/user_profile çš„å®Œæ•´ç”»åƒæœåŠ¡
4. ä½¿ç”¨ LLM ä»å¯¹è¯ä¸Šä¸‹æ–‡å­¦ä¹ ç”¨æˆ·åå¥½

è¿è¡Œæ–¹å¼ï¼š
    python examples/qwen_userprofile_example.py
    python examples/qwen_userprofile_example.py --provider dashscope --model qwen-plus
    python examples/qwen_userprofile_example.py --provider openai --model gpt-4o
"""

import asyncio
import json
import sys
import argparse
from pathlib import Path
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° path
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from app.services.llm_adapter import LLMAdapterImpl, LLMCall, SUPPORTED_PROVIDERS
from app.services.user_profile_impl import UserProfileService, ScenarioRiskLevel
from app.models.schemas import Message


# ============== Prompt æ¨¡æ¿ ==============

PERSONA_ANALYSIS_PROMPT = """ä½ æ˜¯ä¸€ä¸ªç”¨æˆ·ç”»åƒåˆ†æä¸“å®¶ã€‚è¯·æ ¹æ®ä»¥ä¸‹å¯¹è¯å†å²ï¼Œåˆ†æç”¨æˆ·çš„æ€§æ ¼ç‰¹å¾å’Œå½“å‰å¯¹è¯åœºæ™¯ã€‚

## å¯¹è¯å†å²
{conversation}

## åˆ†æç»´åº¦

### 1. ç”¨æˆ·é£æ ¼ (style)
- ç†æ€§ï¼šé€»è¾‘æ¸…æ™°ï¼Œæ³¨é‡äº‹å®å’Œæ•°æ®
- æ„Ÿæ€§ï¼šæƒ…æ„Ÿä¸°å¯Œï¼Œæ³¨é‡æ„Ÿå—å’Œä½“éªŒ
- å¹½é»˜ï¼šè½»æ¾è¯™è°ï¼Œå–„äºè°ƒèŠ‚æ°”æ°›
- å…‹åˆ¶ï¼šå†…æ•›å«è“„ï¼Œè¡¨è¾¾è°¨æ…

### 2. äº¤æµèŠ‚å¥ (pacing)
- slowï¼šå–œæ¬¢æ·±å…¥äº¤æµï¼Œä¸æ€¥äºæ¨è¿›
- normalï¼šæ­£å¸¸èŠ‚å¥ï¼Œé€‚åº¦æ¨è¿›
- fastï¼šå–œæ¬¢å¿«èŠ‚å¥ï¼Œç›´æ¥é«˜æ•ˆ

### 3. é£é™©åå¥½ (risk_tolerance)
- lowï¼šä¿å®ˆè°¨æ…ï¼Œé¿å…å†’é™©è¯é¢˜
- mediumï¼šé€‚åº¦å¼€æ”¾ï¼Œå¯æ¥å—ä¸€å®šé£é™©
- highï¼šå¤§èƒ†å¼€æ”¾ï¼Œæ„¿æ„å°è¯•æ–°è¯é¢˜

### 4. åœºæ™¯é£é™©ç­‰çº§ (risk_level)
- safeï¼šé™Œç”Ÿé˜¶æ®µï¼Œä½å®¹é”™ï¼Œéœ€è¦ä¿å®ˆç­–ç•¥
- balancedï¼šæ¨è¿›å…³ç³»é˜¶æ®µï¼Œå¯ä»¥é€‚åº¦å†’é™©
- riskyï¼šå…³ç³»äº²å¯†ï¼Œé«˜å®¹é”™ï¼Œå¯ä»¥å¤§èƒ†å°è¯•
- recoveryï¼šå…³ç³»ä¿®å¤é˜¶æ®µï¼Œéœ€è¦ç¼“å’Œç­–ç•¥

### 5. å…³ç³»é˜¶æ®µ (relationship_stage)
- strangerï¼šé™Œç”Ÿäºº
- acquaintanceï¼šç†Ÿäºº
- friendï¼šæœ‹å‹
- intimateï¼šäº²å¯†å…³ç³»

### 6. æ¨èç­–ç•¥ (recommended_strategies)
æ ¹æ®åœºæ™¯é€‰æ‹©åˆé€‚çš„ç­–ç•¥ï¼š
- Safe ç­–ç•¥ï¼šsituational_comment, light_humor, neutral_open_question, empathetic_ack, pace_matching
- Balanced ç­–ç•¥ï¼šplayful_tease, direct_compliment, emotional_resonance, story_snippet, flirt_with_escape
- Recovery ç­–ç•¥ï¼štension_release, boundary_respect, misstep_repair

## è¾“å‡ºæ ¼å¼
è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ JSON æ ¼å¼è¾“å‡ºï¼Œä¸è¦åŒ…å«å…¶ä»–å†…å®¹ï¼š
{{
    "style": "ç†æ€§|æ„Ÿæ€§|å¹½é»˜|å…‹åˆ¶",
    "pacing": "slow|normal|fast",
    "risk_tolerance": "low|medium|high",
    "risk_level": "safe|balanced|risky|recovery",
    "relationship_stage": "stranger|acquaintance|friend|intimate",
    "emotional_tone": "positive|neutral|negative|tense",
    "recommended_strategies": ["ç­–ç•¥1", "ç­–ç•¥2", "ç­–ç•¥3"],
    "avoid_patterns": ["éœ€è¦å›é¿çš„æ¨¡å¼"],
    "confidence": 0.0-1.0,
    "analysis": "ç®€è¦åˆ†æè¯´æ˜"
}}
"""


def format_conversation(messages: list[Message]) -> str:
    """æ ¼å¼åŒ–å¯¹è¯å†å²ä¸ºæ–‡æœ¬"""
    lines = []
    for msg in messages:
        speaker = "ç”¨æˆ·" if msg.speaker == "user" else "å¯¹æ–¹"
        lines.append(f"{speaker}: {msg.content}")
    return "\n".join(lines)


def parse_llm_response(response_text: str) -> dict:
    """è§£æ LLM è¿”å›çš„ JSON å“åº”"""
    text = response_text.strip()
    
    # å¤„ç† markdown ä»£ç å—
    if "```json" in text:
        start = text.find("```json") + 7
        end = text.find("```", start)
        text = text[start:end].strip()
    elif "```" in text:
        start = text.find("```") + 3
        end = text.find("```", start)
        text = text[start:end].strip()
    
    # æå– JSON å¯¹è±¡
    if "{" in text:
        start = text.find("{")
        end = text.rfind("}") + 1
        text = text[start:end]
    
    try:
        return json.loads(text)
    except json.JSONDecodeError as e:
        print(f"âš ï¸  JSON è§£æå¤±è´¥: {e}")
        return {
            "style": "ç†æ€§",
            "pacing": "normal",
            "risk_tolerance": "medium",
            "risk_level": "safe",
            "relationship_stage": "stranger",
            "emotional_tone": "neutral",
            "recommended_strategies": ["pace_matching", "empathetic_ack"],
            "avoid_patterns": [],
            "confidence": 0.5,
            "analysis": "è§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼"
        }


async def analyze_with_llm(
    llm_adapter: LLMAdapterImpl,
    user_id: str,
    messages: list[Message],
    provider: str | None = None,
    model: str | None = None,
) -> dict:
    """ä½¿ç”¨ LLM åˆ†æç”¨æˆ·ç”»åƒå’Œåœºæ™¯
    
    Args:
        llm_adapter: LLM é€‚é…å™¨
        user_id: ç”¨æˆ·ID
        messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
        provider: å¯é€‰ï¼ŒæŒ‡å®šå¹³å° (dashscope/openai/gemini ç­‰)
        model: å¯é€‰ï¼ŒæŒ‡å®šæ¨¡å‹ (qwen-plus/gpt-4o ç­‰)
    """
    conversation_text = format_conversation(messages)
    prompt = PERSONA_ANALYSIS_PROMPT.format(conversation=conversation_text)
    
    print("\nğŸ“ å‘é€åˆ†æè¯·æ±‚åˆ° LLM...")
    print(f"   å¯¹è¯è½®æ•°: {len(messages)}")
    
    # åˆ›å»º LLM è°ƒç”¨è¯·æ±‚
    llm_call = LLMCall(
        task_type="persona",
        prompt=prompt,
        quality="normal",
        user_id=user_id,
        provider='dashscope',
        model='qwen-flash',
    )
    
    result = await llm_adapter.call(llm_call)
    
    print(f"   å¹³å°: {result.provider}")
    print(f"   æ¨¡å‹: {result.model}")
    print(f"   Token: {result.input_tokens} + {result.output_tokens}")
    print(f"   æˆæœ¬: ${result.cost_usd:.6f}")
    
    parsed = parse_llm_response(result.text)
    parsed["raw_response"] = result.text
    
    return parsed


async def run_example(provider: str | None = None, model: str | None = None, verbose: bool = False):
    """è¿è¡Œç¤ºä¾‹
    
    Args:
        provider: å¯é€‰ï¼ŒæŒ‡å®šå¹³å°
        model: å¯é€‰ï¼ŒæŒ‡å®šæ¨¡å‹
        verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†è¾“å‡º
    """
    print("=" * 60)
    print("ğŸ¯ LLM + UserProfile å®Œæ•´ç¤ºä¾‹")
    print("=" * 60)
    
    # æ˜¾ç¤ºä½¿ç”¨çš„å¹³å°ä¿¡æ¯
    if provider and model:
        print(f"\nğŸ“Œ ä½¿ç”¨æŒ‡å®šå¹³å°: {provider} / {model}")
    else:
        print("\nğŸ“Œ ä½¿ç”¨é»˜è®¤è·¯ç”± (åŸºäº quality è‡ªåŠ¨é€‰æ‹©)")
    
    print(f"   æ”¯æŒçš„å¹³å°: {SUPPORTED_PROVIDERS}")
    
    # åˆå§‹åŒ– LLM Adapter
    llm_adapter = LLMAdapterImpl()
    
    # ä½¿ç”¨ LLM adapter åˆå§‹åŒ– UserProfileService
    user_profile_service = UserProfileService(llm_adapter=llm_adapter)
    
    user_id = "demo_user_001"
    conversation_id = "conv_001"
    
    # ç¤ºä¾‹å¯¹è¯
    sample_messages = [
        Message(
            id="msg_001",
            speaker="target",
            content="ä½ å¥½å‘€ï¼Œçœ‹åˆ°ä½ çš„èµ„æ–™è§‰å¾—æŒºæœ‰æ„æ€çš„",
            timestamp=datetime.now()
        ),
        Message(
            id="msg_002",
            speaker="user",
            content="å“ˆå“ˆè°¢è°¢ï¼ä½ çš„ç…§ç‰‡ä¹Ÿå¾ˆå¥½çœ‹ï¼Œæ˜¯åœ¨å“ªé‡Œæ‹çš„ï¼Ÿ",
            timestamp=datetime.now()
        ),
        Message(
            id="msg_003",
            speaker="target",
            content="æ˜¯ä¸Šä¸ªæœˆå»äº‘å—æ—…æ¸¸çš„æ—¶å€™æ‹çš„ï¼Œé‚£è¾¹é£æ™¯ç‰¹åˆ«ç¾",
            timestamp=datetime.now()
        ),
        Message(
            id="msg_004",
            speaker="user",
            content="äº‘å—ç¡®å®ä¸é”™ï¼Œæˆ‘ä¹‹å‰å»è¿‡å¤§ç†ï¼Œæ´±æµ·è¾¹éª‘è½¦ç‰¹åˆ«èˆ’æœã€‚ä½ å–œæ¬¢æ—…æ¸¸å—ï¼Ÿ",
            timestamp=datetime.now()
        ),
        Message(
            id="msg_005",
            speaker="target",
            content="è¶…å–œæ¬¢çš„ï¼æ¯å¹´éƒ½ä¼šå®‰æ’å‡ æ¬¡å‡ºè¡Œï¼Œä½ å‘¢ï¼Ÿ",
            timestamp=datetime.now()
        ),
        Message(
            id="msg_006",
            speaker="user",
            content="æˆ‘ä¹Ÿæ˜¯ï¼Œä¸è¿‡å·¥ä½œæ¯”è¾ƒå¿™ï¼Œä¸€èˆ¬å°±å‘¨æœ«çŸ­é€”æ¸¸ã€‚å¯¹äº†ï¼Œä½ å¹³æ—¶é™¤äº†æ—…æ¸¸è¿˜æœ‰ä»€ä¹ˆçˆ±å¥½ï¼Ÿ",
            timestamp=datetime.now()
        ),
        Message(
            id="msg_007",
            speaker="target",
            content="æˆ‘å–œæ¬¢çœ‹ä¹¦å’Œåšé¥­ï¼Œå‘¨æœ«ä¼šå°è¯•åšä¸€äº›æ–°èœå¼",
            timestamp=datetime.now()
        ),
        Message(
            id="msg_008",
            speaker="user",
            content="å“‡ï¼Œä¼šåšé¥­å¤ªæ£’äº†ï¼æˆ‘å¨è‰ºä¸€èˆ¬ï¼Œä¸è¿‡å¾ˆå–œæ¬¢åƒğŸ˜„ ä½ æœ€æ‹¿æ‰‹çš„èœæ˜¯ä»€ä¹ˆï¼Ÿ",
            timestamp=datetime.now()
        ),
    ]
    
    print("\nğŸ“œ å¯¹è¯å†å²:")
    print("-" * 40)
    for msg in sample_messages:
        speaker = "ğŸ‘¤ ç”¨æˆ·" if msg.speaker == "user" else "ğŸ‘© å¯¹æ–¹"
        print(f"{speaker}: {msg.content}")
    print("-" * 40)
    
    try:
        # 1. ä½¿ç”¨ LLM åˆ†æåœºæ™¯
        print("\n" + "=" * 50)
        print("ğŸ“Š ç¬¬ä¸€æ­¥: åœºæ™¯åˆ†æ")
        print("=" * 50)
        
        analysis = await analyze_with_llm(
            llm_adapter=llm_adapter,
            user_id=user_id,
            messages=sample_messages,
            provider=provider,
            model=model,
        )
        
        print("\nğŸ“Š LLM åˆ†æç»“æœ:")
        print("-" * 40)
        print(f"   æ²Ÿé€šé£æ ¼: {analysis.get('style', 'N/A')}")
        print(f"   äº¤æµèŠ‚å¥: {analysis.get('pacing', 'N/A')}")
        print(f"   é£é™©åå¥½: {analysis.get('risk_tolerance', 'N/A')}")
        print(f"   åœºæ™¯é£é™©: {analysis.get('risk_level', 'N/A')}")
        print(f"   å…³ç³»é˜¶æ®µ: {analysis.get('relationship_stage', 'N/A')}")
        print(f"   æƒ…ç»ªåŸºè°ƒ: {analysis.get('emotional_tone', 'N/A')}")
        print(f"   ç½®ä¿¡åº¦: {analysis.get('confidence', 0):.2f}")
        print(f"   åˆ†æè¯´æ˜: {analysis.get('analysis', 'N/A')}")
        
        print("\n   æ¨èç­–ç•¥:")
        for strategy in analysis.get('recommended_strategies', []):
            print(f"     - {strategy}")
        
        if analysis.get('avoid_patterns'):
            print("\n   éœ€è¦å›é¿:")
            for pattern in analysis.get('avoid_patterns', []):
                print(f"     - {pattern}")
        
        # 2. æ›´æ–° UserProfile æœåŠ¡
        risk_level_map = {
            "safe": ScenarioRiskLevel.SAFE,
            "balanced": ScenarioRiskLevel.BALANCED,
            "risky": ScenarioRiskLevel.RISKY,
            "recovery": ScenarioRiskLevel.RECOVERY,
        }
        risk_level = risk_level_map.get(
            analysis.get('risk_level', 'safe'),
            ScenarioRiskLevel.SAFE
        )
        
        # è®¾ç½®æ˜¾å¼æ ‡ç­¾
        await user_profile_service.set_explicit_tags(
            user_id=user_id,
            style=[analysis.get('style', 'ç†æ€§')],
            role=["çº¦ä¼šå¯¹è±¡"],
            intimacy=50.0,
        )

        # ä½¿ç”¨ LLM åˆ†æåœºæ™¯å¹¶æ›´æ–°ï¼ˆç°åœ¨ analyze_scenario ä¼šè‡ªåŠ¨è°ƒç”¨ LLMï¼‰
        print("\nğŸ“ è°ƒç”¨ LLM åˆ†æåœºæ™¯...")
        profile = await user_profile_service.analyze_scenario(
            user_id=user_id,
            conversation_id=conversation_id,
            messages=sample_messages,
            provider='dashscope',
            model='qwen-flash'
        )
        
        # è·å–åœºæ™¯åˆ†æç»“æœ
        if profile.core_profile and profile.core_profile.session_state:
            scenario = profile.core_profile.session_state.scenario
            print("\nğŸ“Š åœºæ™¯åˆ†æç»“æœ:")
            print(f"   é£é™©ç­‰çº§: {scenario.risk_level.value}")
            print(f"   å…³ç³»é˜¶æ®µ: {scenario.relationship_stage}")
            print(f"   æƒ…ç»ªåŸºè°ƒ: {scenario.emotional_tone}")
            print(f"   æ¨èç­–ç•¥: {scenario.recommended_strategies}")
            if scenario.avoid_patterns:
                print(f"   éœ€è¦å›é¿: {scenario.avoid_patterns}")
        
        print("\nâœ… ç”¨æˆ·ç”»åƒå·²æ›´æ–°")
        print(f"   user_id: {profile.user_id}")
        print(f"   style: {profile.style}")
        print(f"   pacing: {profile.pacing}")
        print(f"   risk_tolerance: {profile.risk_tolerance}")
        
        # 3. ä½¿ç”¨ LLM ä»å¯¹è¯å­¦ä¹ ç”¨æˆ·åå¥½
        print("\n" + "=" * 50)
        print("ğŸ§  ç¬¬äºŒæ­¥: ä»å¯¹è¯å­¦ä¹ ç”¨æˆ·åå¥½")
        print("=" * 50)
        
        print("\nğŸ“ è°ƒç”¨ LLM åˆ†æå¯¹è¯åå¥½...")
        learned_preferences = await user_profile_service.learn_preferences_from_conversation(
            user_id=user_id,
            messages=sample_messages,
        )
        
        print(f"\nğŸ“‹ å­¦ä¹ åˆ°çš„åå¥½ (å…± {len(learned_preferences)} é¡¹):")
        print("-" * 40)
        for pref in learned_preferences:
            print(f"   {pref.key}:")
            print(f"     å€¼: {pref.value:.2f}")
            print(f"     ç½®ä¿¡åº¦: {pref.confidence:.2f}")
            print(f"     æ¥æº: {pref.source.value}")
        
        # 4. è·å–æ‰€æœ‰å­¦ä¹ åˆ°çš„åå¥½
        all_preferences = await user_profile_service.get_learned_preferences(user_id)
        print(f"\nğŸ“Š ç”¨æˆ·åå¥½æ±‡æ€» (å…± {len(all_preferences)} é¡¹):")
        for pref in all_preferences:
            bar = "â–ˆ" * int(pref.value * 10) + "â–‘" * (10 - int(pref.value * 10))
            print(f"   {pref.key}: [{bar}] {pref.value:.2f}")
        
        # 5. è·å– LLM å‹å¥½çš„ç”»åƒ
        print("\n" + "=" * 50)
        print("ğŸ“‹ ç¬¬ä¸‰æ­¥: åºåˆ—åŒ–è¾“å‡º")
        print("=" * 50)
        
        llm_profile = await user_profile_service.get_profile_for_llm(user_id)
        if llm_profile:
            print("\nğŸ“‹ LLM å‹å¥½çš„ç”»åƒæ ¼å¼:")
            print("-" * 40)
            print(json.dumps(llm_profile, ensure_ascii=False, indent=2))
        
        # 6. è·å–æ¨èç­–ç•¥
        strategies = await user_profile_service.get_recommended_strategies(user_id)
        if strategies:
            print("\nğŸ¯ å½“å‰æ¨èç­–ç•¥:")
            for s in strategies:
                print(f"   - {s}")
        
        # 7. åºåˆ—åŒ–ä¸º Prompt æ ¼å¼
        prompt_text = await user_profile_service.serialize_to_prompt(user_id)
        if prompt_text:
            print("\nğŸ“ Prompt æ ¼å¼ç”»åƒ:")
            print("-" * 40)
            print(prompt_text)
        
        # 8. æ˜¾ç¤ºä½¿ç”¨ç»Ÿè®¡
        usage = llm_adapter.get_user_usage(user_id)
        print("\nğŸ“ˆ LLM ä½¿ç”¨ç»Ÿè®¡:")
        print(f"   æ€»è°ƒç”¨æ¬¡æ•°: {usage['total_calls']}")
        print(f"   æ€»è¾“å…¥ Token: {usage['total_input_tokens']}")
        print(f"   æ€»è¾“å‡º Token: {usage['total_output_tokens']}")
        print(f"   æ€»æˆæœ¬: ${usage['total_cost_usd']:.6f}")
        
        # æ˜¾ç¤ºåŸå§‹å“åº”ï¼ˆè°ƒè¯•ç”¨ï¼‰
        if verbose:
            print("\nğŸ” LLM åŸå§‹å“åº”:")
            print("-" * 40)
            print(analysis.get("raw_response", "N/A"))
        
    except Exception as e:
        print(f"\nâŒ æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    print("\n" + "=" * 60)
    print("ç¤ºä¾‹å®Œæˆ")
    print("=" * 60)


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="ä½¿ç”¨ LLM æ„å»º UserProfile çš„ç¤ºä¾‹",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # ä½¿ç”¨é»˜è®¤è·¯ç”±
  python examples/qwen_userprofile_example.py
  
  # æŒ‡å®šä½¿ç”¨ DashScope çš„ qwen-plus
  python examples/qwen_userprofile_example.py --provider dashscope --model qwen-plus
  
  # æŒ‡å®šä½¿ç”¨ OpenAI çš„ gpt-4o
  python examples/qwen_userprofile_example.py --provider openai --model gpt-4o
  
  # æŒ‡å®šä½¿ç”¨ Gemini
  python examples/qwen_userprofile_example.py --provider gemini --model gemini-1.5-flash
  
  # æ˜¾ç¤ºè¯¦ç»†è¾“å‡º
  python examples/qwen_userprofile_example.py --verbose
        """
    )
    parser.add_argument(
        "--provider",
        type=str,
        choices=SUPPORTED_PROVIDERS,
        help=f"æŒ‡å®š LLM å¹³å°: {SUPPORTED_PROVIDERS}",
    )
    parser.add_argument(
        "--model",
        type=str,
        help="æŒ‡å®šæ¨¡å‹åç§° (å¦‚ qwen-plus, gpt-4o, gemini-1.5-flash)",
    )
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="æ˜¾ç¤ºè¯¦ç»†è¾“å‡º",
    )
    return parser.parse_args()


if __name__ == "__main__":
    args = parse_args()
    
    # éªŒè¯å‚æ•°
    if args.provider and not args.model:
        print("âŒ é”™è¯¯: æŒ‡å®š --provider æ—¶å¿…é¡»åŒæ—¶æŒ‡å®š --model")
        sys.exit(1)
    if args.model and not args.provider:
        print("âŒ é”™è¯¯: æŒ‡å®š --model æ—¶å¿…é¡»åŒæ—¶æŒ‡å®š --provider")
        sys.exit(1)
    
    asyncio.run(run_example(
        provider=args.provider,
        model=args.model,
        verbose=args.verbose,
    ))
