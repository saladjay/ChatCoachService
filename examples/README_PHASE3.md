# Phase 3 Token Analysis Examples

è¿™ä¸ªç›®å½•åŒ…å«ç”¨äºåˆ†æå’Œå¯¹æ¯” Phase 3 ä¼˜åŒ–æ•ˆæœçš„ç¤ºä¾‹è„šæœ¬ã€‚

---

## å¿«é€Ÿå¼€å§‹

### 1. è¿è¡Œå®Œæ•´åˆ†æç¤ºä¾‹

```bash
# è¿è¡Œå®Œæ•´çš„ token åˆ†æç¤ºä¾‹
python examples/phase3_token_analysis_example.py
```

è¿™ä¸ªè„šæœ¬ä¼šï¼š
- âœ… ä½¿ç”¨ baseline é…ç½®è¿è¡Œå®Œæ•´æµç¨‹
- âœ… ä½¿ç”¨ optimized é…ç½®è¿è¡Œå®Œæ•´æµç¨‹
- âœ… ç”Ÿæˆä¸¤ä¸ª trace æ–‡ä»¶ï¼ˆ`logs/trace_baseline.jsonl` å’Œ `logs/trace_optimized.jsonl`ï¼‰
- âœ… è‡ªåŠ¨åˆ†æå’Œå¯¹æ¯” token ä½¿ç”¨æƒ…å†µ
- âœ… æ˜¾ç¤ºè¯¦ç»†çš„ prompt å’Œ response å†…å®¹

### 2. åˆ†æå·²æœ‰çš„ trace æ–‡ä»¶

```bash
# åˆ†æå•ä¸ªæ–‡ä»¶
python scripts/analyze_trace.py logs/trace.jsonl

# å¯¹æ¯”ä¸¤ä¸ªæ–‡ä»¶
python scripts/analyze_trace.py logs/trace_baseline.jsonl logs/trace_optimized.jsonl --compare

# æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯ï¼ˆåŒ…å«å®Œæ•´ prompt å’Œ responseï¼‰
python scripts/analyze_trace.py logs/trace.jsonl --detailed
```

---

## è¾“å‡ºç¤ºä¾‹

### å¯¹æ¯”æŠ¥å‘Šç¤ºä¾‹

```
================================================================================
TOKEN USAGE COMPARISON REPORT
================================================================================

ğŸ“Š OVERALL COMPARISON
--------------------------------------------------------------------------------
Metric                         Baseline        Optimized       Change         
--------------------------------------------------------------------------------
Total Tokens                   2,450           1,225           +50.0%
Input Tokens                   1,800           1,200           +33.3%
Output Tokens                  650             325             +50.0%
Number of LLM Calls            3               3               

ğŸ“‹ PER-CALL BREAKDOWN
--------------------------------------------------------------------------------

Call #1: scene
  Model: qwen-flash
  Input Tokens:    800 â†’    600 (+25.0%)
  Output Tokens:   150 â†’     75 (+50.0%)

Call #2: generation
  Model: qwen-flash
  Input Tokens:    900 â†’    500 (+44.4%)
  Output Tokens:   450 â†’    225 (+50.0%)
```

### è¯¦ç»†è°ƒç”¨ä¿¡æ¯ç¤ºä¾‹

```
================================================================================
LLM CALL #1: SCENE
================================================================================

ğŸ“Œ Metadata:
  Provider: dashscope
  Model: qwen-flash
  Timestamp: 2026-01-22T10:30:45.123456

ğŸ“Š Token Usage:
  Input Tokens:    800
  Output Tokens:   150
  Total Tokens:    950
  Cost (USD):    $0.004750

ğŸ“ Prompt (Input):
--------------------------------------------------------------------------------
You are a conversation coach analyzing a dating conversation...
[å®Œæ•´çš„ prompt å†…å®¹]

ğŸ’¬ Response (Output):
--------------------------------------------------------------------------------
{"rs":"I","scn":"B","il":50,"rf":[],"cs":"S","rsc":"B","rst":["emotional_resonance"]}
```

---

## é…ç½®é€‰é¡¹

### Baseline é…ç½®ï¼ˆæœªä¼˜åŒ–ï¼‰

```python
PromptConfig(
    include_reasoning=True,      # åŒ…å«æ¨ç†å­—æ®µ
    max_reply_tokens=200,        # è¾ƒé•¿çš„å›å¤
    use_compact_schemas=False    # ä½¿ç”¨å®Œæ•´æ¨¡å¼
)
```

### Optimized é…ç½®ï¼ˆPhase 3 ä¼˜åŒ–ï¼‰

```python
PromptConfig(
    include_reasoning=False,     # æ’é™¤æ¨ç†å­—æ®µ â†’ èŠ‚çœ ~40% è¾“å‡º token
    max_reply_tokens=100,        # é€‚ä¸­çš„å›å¤é•¿åº¦ â†’ èŠ‚çœ ~20% è¾“å‡º token
    use_compact_schemas=True     # ä½¿ç”¨ç´§å‡‘æ¨¡å¼ â†’ èŠ‚çœ ~30% è¾“å‡º token
)
```

**é¢„æœŸæ€»èŠ‚çœ**: ~50% è¾“å‡º token

---

## è‡ªå®šä¹‰æµ‹è¯•

### ç¤ºä¾‹ 1: æµ‹è¯•ä¸åŒçš„ max_reply_tokens

```python
import asyncio
from app.core.config import PromptConfig
from examples.phase3_token_analysis_example import run_complete_flow_with_config

async def test_different_lengths():
    configs = [
        ("short", 50),
        ("normal", 100),
        ("long", 200),
    ]
    
    for name, max_tokens in configs:
        await run_complete_flow_with_config(
            user_id="test_user",
            conversation=messages,
            prompt_config=PromptConfig(max_reply_tokens=max_tokens),
            trace_file=f"logs/trace_{name}.jsonl"
        )

asyncio.run(test_different_lengths())
```

### ç¤ºä¾‹ 2: æµ‹è¯•æ¨ç†æ§åˆ¶çš„å½±å“

```python
async def test_reasoning_impact():
    # åŒ…å«æ¨ç†
    await run_complete_flow_with_config(
        user_id="test_user",
        conversation=messages,
        prompt_config=PromptConfig(include_reasoning=True),
        trace_file="logs/trace_with_reasoning.jsonl"
    )
    
    # ä¸åŒ…å«æ¨ç†
    await run_complete_flow_with_config(
        user_id="test_user",
        conversation=messages,
        prompt_config=PromptConfig(include_reasoning=False),
        trace_file="logs/trace_without_reasoning.jsonl"
    )

asyncio.run(test_reasoning_impact())
```

---

## æ–‡ä»¶è¯´æ˜

| æ–‡ä»¶ | è¯´æ˜ |
|------|------|
| `phase3_token_analysis_example.py` | å®Œæ•´çš„ token åˆ†æç¤ºä¾‹è„šæœ¬ |
| `../scripts/analyze_trace.py` | Trace æ–‡ä»¶åˆ†æå·¥å…· |
| `../PHASE3_USAGE_GUIDE.md` | è¯¦ç»†ä½¿ç”¨æŒ‡å— |
| `README_PHASE3.md` | æœ¬æ–‡ä»¶ |

---

## å¸¸è§é—®é¢˜

### Q: å¦‚ä½•å¯ç”¨ trace æ—¥å¿—ï¼Ÿ

A: åœ¨ `.env` æ–‡ä»¶ä¸­è®¾ç½®ï¼š
```bash
TRACE_ENABLED=true
TRACE_LOG_LLM_PROMPT=true
TRACE_FILE_PATH=logs/trace.jsonl
```

### Q: Trace æ–‡ä»¶åœ¨å“ªé‡Œï¼Ÿ

A: é»˜è®¤åœ¨ `logs/` ç›®å½•ä¸‹ï¼Œæ–‡ä»¶åä¸º `trace.jsonl` æˆ–è‡ªå®šä¹‰çš„åç§°ã€‚

### Q: å¦‚ä½•æŸ¥çœ‹å®Œæ•´çš„ prompt å’Œ responseï¼Ÿ

A: ä½¿ç”¨ `--detailed` æ ‡å¿—ï¼š
```bash
python scripts/analyze_trace.py logs/trace.jsonl --detailed
```

### Q: ä¸ºä»€ä¹ˆ token å‡å°‘ä¸æ˜æ˜¾ï¼Ÿ

A: æ£€æŸ¥ä»¥ä¸‹å‡ ç‚¹ï¼š
1. ç¡®è®¤é…ç½®æ­£ç¡®åº”ç”¨ï¼ˆ`include_reasoning=False`ï¼‰
2. ç¡®è®¤ä½¿ç”¨äº†ç´§å‡‘æ¨¡å¼ï¼ˆ`use_compact_schemas=True`ï¼‰
3. ç¡®è®¤ `max_reply_tokens` è®¾ç½®åˆç†
4. æŸ¥çœ‹è¯¦ç»†çš„ trace å¯¹æ¯”ç¡®è®¤å·®å¼‚

---

## ä¸‹ä¸€æ­¥

1. è¿è¡Œç¤ºä¾‹è„šæœ¬æŸ¥çœ‹æ•ˆæœ
2. é˜…è¯» [è¯¦ç»†ä½¿ç”¨æŒ‡å—](../PHASE3_USAGE_GUIDE.md)
3. æ ¹æ®å®é™…éœ€æ±‚è°ƒæ•´é…ç½®
4. åœ¨ç”Ÿäº§ç¯å¢ƒä¸­åº”ç”¨ä¼˜åŒ–

---

**ç›¸å…³æ–‡æ¡£**:
- [Phase 3 å®ŒæˆæŠ¥å‘Š](../PHASE3_COMPLETION_REPORT.md)
- [Phase 3 å¿«é€Ÿæ€»ç»“](../PHASE3_SUMMARY.md)
- [è¯¦ç»†ä½¿ç”¨æŒ‡å—](../PHASE3_USAGE_GUIDE.md)
