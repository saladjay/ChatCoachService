# Phase 3 Usage Guide: Token Analysis and Comparison

æœ¬æŒ‡å—ä»‹ç»å¦‚ä½•ä½¿ç”¨ Phase 3 çš„å·¥å…·æ¥åˆ†æå’Œå¯¹æ¯”ä¸åŒé…ç½®ä¸‹çš„ token ä½¿ç”¨æƒ…å†µã€‚

---

## ç›®å½•

1. [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
2. [è¿è¡Œå®Œæ•´æµç¨‹ç¤ºä¾‹](#è¿è¡Œå®Œæ•´æµç¨‹ç¤ºä¾‹)
3. [åˆ†æ Trace æ–‡ä»¶](#åˆ†æ-trace-æ–‡ä»¶)
4. [é…ç½®é€‰é¡¹](#é…ç½®é€‰é¡¹)
5. [å®é™…æ¡ˆä¾‹](#å®é™…æ¡ˆä¾‹)

---

## å¿«é€Ÿå¼€å§‹

### 1. å¯ç”¨ Trace æ—¥å¿—

åœ¨ `.env` æ–‡ä»¶ä¸­å¯ç”¨ trace æ—¥å¿—ï¼š

```bash
# Trace Configuration
TRACE_ENABLED=true
TRACE_LEVEL=info
TRACE_FILE_PATH=logs/trace.jsonl
TRACE_LOG_LLM_PROMPT=true
```

### 2. è¿è¡Œç¤ºä¾‹è„šæœ¬

```bash
# è¿è¡Œ Phase 3 token åˆ†æç¤ºä¾‹
python examples/phase3_token_analysis_example.py
```

è¿™ä¸ªè„šæœ¬ä¼šï¼š
- ä½¿ç”¨ä¸¤ç§é…ç½®è¿è¡Œå®Œæ•´æµç¨‹ï¼ˆbaseline å’Œ optimizedï¼‰
- ç”Ÿæˆä¸¤ä¸ª trace æ–‡ä»¶
- è‡ªåŠ¨åˆ†æå’Œå¯¹æ¯” token ä½¿ç”¨æƒ…å†µ
- æ˜¾ç¤ºè¯¦ç»†çš„è¾“å…¥è¾“å‡ºå†…å®¹

---

## è¿è¡Œå®Œæ•´æµç¨‹ç¤ºä¾‹

### ç¤ºä¾‹ 1: åŸºç¡€é…ç½® vs ä¼˜åŒ–é…ç½®

```python
import asyncio
from app.core.config import PromptConfig
from examples.phase3_token_analysis_example import run_complete_flow_with_config

# åŸºç¡€é…ç½®ï¼ˆåŒ…å«æ¨ç†ï¼Œè¾ƒé•¿å›å¤ï¼‰
baseline_config = PromptConfig(
    include_reasoning=True,
    max_reply_tokens=200,
    use_compact_schemas=False
)

# ä¼˜åŒ–é…ç½®ï¼ˆPhase 3 ä¼˜åŒ–ï¼‰
optimized_config = PromptConfig(
    include_reasoning=False,
    max_reply_tokens=100,
    use_compact_schemas=True
)

# è¿è¡Œå¹¶å¯¹æ¯”
asyncio.run(run_complete_flow_with_config(
    user_id="test_user",
    conversation=messages,
    prompt_config=baseline_config,
    trace_file="logs/trace_baseline.jsonl"
))

asyncio.run(run_complete_flow_with_config(
    user_id="test_user",
    conversation=messages,
    prompt_config=optimized_config,
    trace_file="logs/trace_optimized.jsonl"
))
```

### ç¤ºä¾‹ 2: è‡ªå®šä¹‰é…ç½®æµ‹è¯•

```python
# æµ‹è¯•ä¸åŒçš„ max_reply_tokens è®¾ç½®
configs = [
    ("cheap", PromptConfig(max_reply_tokens=50)),
    ("normal", PromptConfig(max_reply_tokens=100)),
    ("premium", PromptConfig(max_reply_tokens=200)),
]

for name, config in configs:
    await run_complete_flow_with_config(
        user_id="test_user",
        conversation=messages,
        prompt_config=config,
        trace_file=f"logs/trace_{name}.jsonl"
    )
```

---

## åˆ†æ Trace æ–‡ä»¶

### å·¥å…· 1: analyze_trace.py

è¿™æ˜¯ä¸€ä¸ªå‘½ä»¤è¡Œå·¥å…·ï¼Œç”¨äºåˆ†æ trace.jsonl æ–‡ä»¶ã€‚

#### åŸºæœ¬ç”¨æ³•

```bash
# åˆ†æå•ä¸ª trace æ–‡ä»¶
python scripts/analyze_trace.py logs/trace.jsonl

# æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯ï¼ˆåŒ…å«å®Œæ•´çš„ prompt å’Œ responseï¼‰
python scripts/analyze_trace.py logs/trace.jsonl --detailed

# å¯¹æ¯”ä¸¤ä¸ª trace æ–‡ä»¶
python scripts/analyze_trace.py logs/trace_baseline.jsonl logs/trace_optimized.jsonl --compare

# å¯¹æ¯”å¹¶æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
python scripts/analyze_trace.py logs/trace_baseline.jsonl logs/trace_optimized.jsonl --compare --detailed
```

#### è¾“å‡ºç¤ºä¾‹

```
================================================================================
COMPARISON REPORT
================================================================================

ğŸ“Š OVERALL COMPARISON
--------------------------------------------------------------------------------
Metric                    Baseline        Optimized       Change         
--------------------------------------------------------------------------------
Total Tokens              2,450           1,225           +50.0%
Input Tokens              1,800           1,200           +33.3%
Output Tokens             650             325             +50.0%
Total Cost (USD)          $0.012300       $0.006150       +50.0%
Number of Calls           3               3               

ğŸ“‹ PER-CALL COMPARISON
--------------------------------------------------------------------------------

Call #1: scene
  Model: qwen-flash
  Input:  800 â†’ 600 (+25.0%)
  Output: 150 â†’ 75 (+50.0%)
  Total:  950 â†’ 675 (+28.9%)

Call #2: generation
  Model: qwen-flash
  Input:  900 â†’ 500 (+44.4%)
  Output: 450 â†’ 225 (+50.0%)
  Total:  1350 â†’ 725 (+46.3%)
```

### å·¥å…· 2: ç¼–ç¨‹æ–¹å¼åˆ†æ

```python
from scripts.analyze_trace import load_trace_file, extract_llm_calls

# åŠ è½½ trace æ–‡ä»¶
entries = load_trace_file("logs/trace.jsonl")
llm_calls = extract_llm_calls(entries)

# è®¡ç®—æ€» token æ•°
total_tokens = sum(call["total_tokens"] for call in llm_calls)
total_cost = sum(call["cost_usd"] for call in llm_calls)

print(f"Total tokens: {total_tokens}")
print(f"Total cost: ${total_cost:.6f}")

# æŸ¥çœ‹æ¯ä¸ªè°ƒç”¨çš„è¯¦ç»†ä¿¡æ¯
for i, call in enumerate(llm_calls, 1):
    print(f"\nCall #{i}: {call['task_type']}")
    print(f"  Input tokens: {call['input_tokens']}")
    print(f"  Output tokens: {call['output_tokens']}")
    print(f"  Prompt preview: {call['prompt'][:100]}...")
```

---

## é…ç½®é€‰é¡¹

### PromptConfig å‚æ•°

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `include_reasoning` | bool | False | æ˜¯å¦åœ¨è¾“å‡ºä¸­åŒ…å«æ¨ç†å­—æ®µ |
| `max_reply_tokens` | int | 100 | å›å¤çš„æœ€å¤§ token æ•°ï¼ˆ20-500ï¼‰ |
| `use_compact_schemas` | bool | True | æ˜¯å¦ä½¿ç”¨ç´§å‡‘çš„è¾“å‡ºæ¨¡å¼ |

### ç¯å¢ƒå˜é‡é…ç½®

```bash
# æ¨ç†æ§åˆ¶
PROMPT_INCLUDE_REASONING=false

# é•¿åº¦çº¦æŸ
PROMPT_MAX_REPLY_TOKENS=100

# æ¨¡å¼å‹ç¼©
PROMPT_USE_COMPACT_SCHEMAS=true
```

### é¢„è®¾é…ç½®

#### 1. æœ€å¤§ä¼˜åŒ–ï¼ˆç”Ÿäº§ç¯å¢ƒæ¨èï¼‰

```python
PromptConfig(
    include_reasoning=False,
    max_reply_tokens=100,
    use_compact_schemas=True
)
```

**é¢„æœŸæ•ˆæœ**:
- è¾“å‡º token å‡å°‘: ~50%
- æˆæœ¬èŠ‚çœ: ~50%
- è´¨é‡: ä¿æŒä¸å˜

#### 2. å¹³è¡¡é…ç½®

```python
PromptConfig(
    include_reasoning=False,
    max_reply_tokens=150,
    use_compact_schemas=True
)
```

**é¢„æœŸæ•ˆæœ**:
- è¾“å‡º token å‡å°‘: ~40%
- æˆæœ¬èŠ‚çœ: ~40%
- è´¨é‡: ç•¥æœ‰æå‡ï¼ˆæ›´è¯¦ç»†çš„å›å¤ï¼‰

#### 3. è°ƒè¯•é…ç½®

```python
PromptConfig(
    include_reasoning=True,
    max_reply_tokens=200,
    use_compact_schemas=False
)
```

**é¢„æœŸæ•ˆæœ**:
- å®Œæ•´çš„æ¨ç†ä¿¡æ¯
- æ›´é•¿çš„å›å¤
- ä¾¿äºè°ƒè¯•å’Œåˆ†æ

#### 4. æˆæœ¬æ•æ„Ÿé…ç½®

```python
PromptConfig(
    include_reasoning=False,
    max_reply_tokens=50,
    use_compact_schemas=True
)
```

**é¢„æœŸæ•ˆæœ**:
- è¾“å‡º token å‡å°‘: ~60%
- æˆæœ¬èŠ‚çœ: ~60%
- è´¨é‡: ç®€æ´ä½†å®Œæ•´

---

## å®é™…æ¡ˆä¾‹

### æ¡ˆä¾‹ 1: å¯¹æ¯”ä¸åŒé…ç½®çš„æ•ˆæœ

```python
"""
æµ‹è¯•åœºæ™¯ï¼šç”¨æˆ·å’¨è¯¢çº¦ä¼šå»ºè®®
ç›®æ ‡ï¼šå¯¹æ¯” baseline å’Œ optimized é…ç½®çš„ token ä½¿ç”¨æƒ…å†µ
"""

import asyncio
from datetime import datetime
from app.models.schemas import Message
from app.core.config import PromptConfig
from examples.phase3_token_analysis_example import run_complete_flow_with_config

async def test_dating_advice():
    conversation = [
        Message(
            id="1",
            speaker="user",
            content="I have a first date tomorrow and I'm nervous.",
            timestamp=datetime.now()
        ),
        Message(
            id="2",
            speaker="assistant",
            content="That's normal! What are you most worried about?",
            timestamp=datetime.now()
        ),
        Message(
            id="3",
            speaker="user",
            content="I'm worried I'll run out of things to talk about.",
            timestamp=datetime.now()
        )
    ]
    
    # Baseline
    baseline_result = await run_complete_flow_with_config(
        user_id="test_user",
        conversation=conversation,
        prompt_config=PromptConfig(
            include_reasoning=True,
            max_reply_tokens=200,
            use_compact_schemas=False
        ),
        trace_file="logs/trace_dating_baseline.jsonl"
    )
    
    # Optimized
    optimized_result = await run_complete_flow_with_config(
        user_id="test_user",
        conversation=conversation,
        prompt_config=PromptConfig(
            include_reasoning=False,
            max_reply_tokens=100,
            use_compact_schemas=True
        ),
        trace_file="logs/trace_dating_optimized.jsonl"
    )
    
    print("âœ… Test complete! Analyze with:")
    print("python scripts/analyze_trace.py logs/trace_dating_baseline.jsonl logs/trace_dating_optimized.jsonl --compare --detailed")

asyncio.run(test_dating_advice())
```

### æ¡ˆä¾‹ 2: æ‰¹é‡æµ‹è¯•ä¸åŒé…ç½®

```python
"""
æ‰¹é‡æµ‹è¯•ä¸åŒçš„ max_reply_tokens è®¾ç½®
"""

import asyncio
from examples.phase3_token_analysis_example import run_complete_flow_with_config

async def batch_test():
    test_configs = [
        ("ultra_short", 50),
        ("short", 75),
        ("normal", 100),
        ("long", 150),
        ("ultra_long", 200),
    ]
    
    for name, max_tokens in test_configs:
        print(f"\n{'='*80}")
        print(f"Testing: {name} (max_tokens={max_tokens})")
        print(f"{'='*80}")
        
        await run_complete_flow_with_config(
            user_id="test_user",
            conversation=messages,
            prompt_config=PromptConfig(
                include_reasoning=False,
                max_reply_tokens=max_tokens,
                use_compact_schemas=True
            ),
            trace_file=f"logs/trace_{name}.jsonl"
        )
    
    print("\nâœ… All tests complete!")
    print("\nAnalyze results:")
    for name, _ in test_configs:
        print(f"  python scripts/analyze_trace.py logs/trace_{name}.jsonl")

asyncio.run(batch_test())
```

### æ¡ˆä¾‹ 3: A/B æµ‹è¯•æ¡†æ¶

```python
"""
A/B æµ‹è¯•æ¡†æ¶ï¼šå¯¹æ¯”ä¸åŒä¼˜åŒ–ç­–ç•¥
"""

import asyncio
from typing import List, Dict
from app.core.config import PromptConfig

class ABTestFramework:
    def __init__(self):
        self.results = []
    
    async def run_test(
        self,
        test_name: str,
        config: PromptConfig,
        conversations: List[List[Message]]
    ):
        """è¿è¡Œå•ä¸ªæµ‹è¯•é…ç½®"""
        total_tokens = 0
        total_cost = 0
        
        for i, conv in enumerate(conversations):
            trace_file = f"logs/ab_test_{test_name}_{i}.jsonl"
            
            result = await run_complete_flow_with_config(
                user_id=f"test_user_{i}",
                conversation=conv,
                prompt_config=config,
                trace_file=trace_file
            )
            
            # åˆ†æç»“æœ
            analysis = analyze_trace_file(trace_file)
            total_tokens += analysis["total_tokens"]
            total_cost += sum(call["cost_usd"] for call in analysis["llm_calls"])
        
        self.results.append({
            "name": test_name,
            "config": config,
            "total_tokens": total_tokens,
            "total_cost": total_cost,
            "avg_tokens_per_conv": total_tokens / len(conversations)
        })
    
    def print_results(self):
        """æ‰“å° A/B æµ‹è¯•ç»“æœ"""
        print("\n" + "="*80)
        print("A/B TEST RESULTS")
        print("="*80)
        
        for result in sorted(self.results, key=lambda x: x["total_tokens"]):
            print(f"\n{result['name']}:")
            print(f"  Total tokens: {result['total_tokens']:,}")
            print(f"  Avg per conversation: {result['avg_tokens_per_conv']:.0f}")
            print(f"  Total cost: ${result['total_cost']:.6f}")
            print(f"  Config: reasoning={result['config'].include_reasoning}, "
                  f"max_tokens={result['config'].max_reply_tokens}")

# ä½¿ç”¨ç¤ºä¾‹
async def run_ab_test():
    framework = ABTestFramework()
    
    # å‡†å¤‡æµ‹è¯•å¯¹è¯
    conversations = [...]  # å¤šä¸ªæµ‹è¯•å¯¹è¯
    
    # æµ‹è¯•ä¸åŒé…ç½®
    await framework.run_test("baseline", PromptConfig(
        include_reasoning=True,
        max_reply_tokens=200,
        use_compact_schemas=False
    ), conversations)
    
    await framework.run_test("optimized", PromptConfig(
        include_reasoning=False,
        max_reply_tokens=100,
        use_compact_schemas=True
    ), conversations)
    
    await framework.run_test("ultra_optimized", PromptConfig(
        include_reasoning=False,
        max_reply_tokens=50,
        use_compact_schemas=True
    ), conversations)
    
    framework.print_results()

asyncio.run(run_ab_test())
```

---

## æœ€ä½³å®è·µ

### 1. å¼€å‘é˜¶æ®µ

- ä½¿ç”¨ `include_reasoning=True` ä¾¿äºè°ƒè¯•
- ä½¿ç”¨ `--detailed` æ ‡å¿—æŸ¥çœ‹å®Œæ•´çš„ prompt å’Œ response
- ä¿å­˜ trace æ–‡ä»¶ç”¨äºåç»­åˆ†æ

### 2. æµ‹è¯•é˜¶æ®µ

- å¯¹æ¯”å¤šç§é…ç½®æ‰¾åˆ°æœ€ä½³å¹³è¡¡ç‚¹
- ä½¿ç”¨çœŸå®å¯¹è¯æ•°æ®è¿›è¡Œæµ‹è¯•
- æµ‹é‡è´¨é‡æŒ‡æ ‡ï¼ˆä¸ä»…ä»…æ˜¯ token æ•°ï¼‰

### 3. ç”Ÿäº§ç¯å¢ƒ

- ä½¿ç”¨ä¼˜åŒ–é…ç½®ï¼ˆ`include_reasoning=False`ï¼‰
- æ ¹æ®è´¨é‡å±‚çº§è®¾ç½®åˆé€‚çš„ `max_reply_tokens`
- å®šæœŸåˆ†æ trace æ–‡ä»¶ç›‘æ§æ€§èƒ½

### 4. ç›‘æ§å’Œä¼˜åŒ–

- å®šæœŸå¯¹æ¯”ä¸åŒæ—¶æœŸçš„ trace æ–‡ä»¶
- è·Ÿè¸ª token ä½¿ç”¨è¶‹åŠ¿
- æ ¹æ®å®é™…æ•ˆæœè°ƒæ•´é…ç½®

---

## æ•…éšœæ’æŸ¥

### é—®é¢˜ 1: Trace æ–‡ä»¶ä¸ºç©º

**åŸå› **: Trace æ—¥å¿—æœªå¯ç”¨

**è§£å†³æ–¹æ¡ˆ**:
```bash
# åœ¨ .env ä¸­è®¾ç½®
TRACE_ENABLED=true
TRACE_LOG_LLM_PROMPT=true
```

### é—®é¢˜ 2: æ— æ³•å¯¹æ¯”ä¸¤ä¸ªæ–‡ä»¶

**åŸå› **: LLM è°ƒç”¨æ¬¡æ•°ä¸åŒ

**è§£å†³æ–¹æ¡ˆ**: ç¡®ä¿ä¸¤ä¸ªé…ç½®ä½¿ç”¨ç›¸åŒçš„è¾“å…¥æ•°æ®å’Œæµç¨‹

### é—®é¢˜ 3: Token å‡å°‘ä¸æ˜æ˜¾

**åŸå› **: å¯èƒ½ä½¿ç”¨äº†é”™è¯¯çš„é…ç½®

**è§£å†³æ–¹æ¡ˆ**: æ£€æŸ¥é…ç½®æ˜¯å¦æ­£ç¡®åº”ç”¨
```python
# éªŒè¯é…ç½®
print(f"include_reasoning: {config.include_reasoning}")
print(f"max_reply_tokens: {config.max_reply_tokens}")
print(f"use_compact_schemas: {config.use_compact_schemas}")
```

---

## ç›¸å…³æ–‡æ¡£

- [Phase 3 å®ŒæˆæŠ¥å‘Š](PHASE3_COMPLETION_REPORT.md)
- [Phase 3 å¿«é€Ÿæ€»ç»“](PHASE3_SUMMARY.md)
- [Token ä¼˜åŒ–å®æ–½æ€»ç»“](TOKEN_OPTIMIZATION_IMPLEMENTATION.md)
- [å®æ–½æ¸…å•](how_to_reduce_token/IMPLEMENTATION_CHECKLIST.md)

---

**æœ€åæ›´æ–°**: 2026-01-22  
**ç‰ˆæœ¬**: 1.0
