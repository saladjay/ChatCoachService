# Application Settings
DEBUG=false

# Feature Flags
# Disable intimacy check (skip validation of reply appropriateness)
NO_INTIMACY_CHECK=false

# Enable merge_step optimized flow (combines screenshot parsing, context building, and scenario analysis)
# When enabled, uses a single LLM call instead of three separate calls for improved performance
# Default: false (uses traditional separate calls)
USE_MERGE_STEP=false

# Log failed JSON parsing replies to logs/failed_json_replies/ directory
# Useful for collecting error cases and improving prompt engineering
LOG_FAILED_JSON_REPLIES=false

# Trace Logging Configuration
# Enable detailed trace logging of all LLM calls and processing steps
TRACE_ENABLED=false

# Trace logging level: error, info, or debug
# - error: Only log errors
# - info: Log major steps and LLM calls (recommended)
# - debug: Log all details including intermediate steps
TRACE_LEVEL=info

# Path to trace log file (JSONL format)
TRACE_FILE_PATH=logs/trace.jsonl

# Log full LLM prompts in trace (useful for debugging, increases log size)
# When enabled with --log-prompt flag, all prompts, responses, tokens, costs, and timing will be logged
TRACE_LOG_LLM_PROMPT=true

# LLM Configuration
LLM_DEFAULT_PROVIDER=openai
LLM_DEFAULT_MODEL=gpt-4
LLM_FALLBACK_MODEL=gpt-3.5-turbo
LLM_CHEAP_MODEL=gpt-3.5-turbo
LLM_PREMIUM_MODEL=gpt-4-turbo

# Multimodal LLM Configuration
# Override the default provider for multimodal (vision) tasks
# Options: openrouter, gemini, dashscope, openai
# If not set, will use the first available provider with a multimodal model
# MULTIMODAL_DEFAULT_PROVIDER=openrouter

# Orchestrator Configuration
ORCHESTRATOR_MAX_RETRIES=3
ORCHESTRATOR_TIMEOUT_SECONDS=30.0
ORCHESTRATOR_RETRY_DELAY_SECONDS=0.5
ORCHESTRATOR_EXPONENTIAL_BACKOFF=true

# Billing Configuration
BILLING_COST_LIMIT_USD=0.1
BILLING_DEFAULT_USER_QUOTA_USD=10.0

# Database Configuration
DB_URL=sqlite+aiosqlite:///./conversation.db
DB_ECHO=false

# CORS Settings (comma-separated for multiple origins)
CORS_ORIGINS=["*"]

# Prompt Optimization Configuration (Phase 3: Output Optimization)
# Control whether reasoning fields are included in LLM outputs
PROMPT_INCLUDE_REASONING=false

# Maximum tokens for reply generation (20-500)
PROMPT_MAX_REPLY_TOKENS=100

# Use compact output schemas to reduce tokens
PROMPT_USE_COMPACT_SCHEMAS=true
