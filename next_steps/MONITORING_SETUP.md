# ç›‘æ§ç³»ç»Ÿè®¾ç½® (Monitoring Setup)

**ç›®æ ‡**: å»ºç«‹å®Œæ•´çš„ç›‘æ§ä½“ç³»  
**é¢„è®¡æ—¶é—´**: 3-5 å¤©  
**ä¼˜å…ˆçº§**: â­â­â­ é«˜

---

## ğŸ“‹ æ¦‚è¿°

ç›‘æ§ç³»ç»Ÿæ˜¯ç”Ÿäº§ç¯å¢ƒçš„å¿…å¤‡ç»„ä»¶ã€‚æœ¬æ–‡æ¡£æè¿°å¦‚ä½•ä¸º Token ä¼˜åŒ–ç³»ç»Ÿå»ºç«‹å®Œæ•´çš„ç›‘æ§ä½“ç³»ã€‚

---

## ğŸ¯ ç›‘æ§ç›®æ ‡

### æ ¸å¿ƒæŒ‡æ ‡
1. **Token ä½¿ç”¨é‡** - å®æ—¶è¿½è¸ª token æ¶ˆè€—
2. **æˆæœ¬** - ç›‘æ§ API è°ƒç”¨æˆæœ¬
3. **è´¨é‡** - è¿½è¸ªå›å¤è´¨é‡æŒ‡æ ‡
4. **æ€§èƒ½** - ç›‘æ§å“åº”æ—¶é—´å’Œé”™è¯¯ç‡

---

## ğŸ“Š ç›‘æ§æ¶æ„

```
åº”ç”¨å±‚
  â†“ (è®°å½•æŒ‡æ ‡)
Trace æ—¥å¿— (trace.jsonl)
  â†“ (è§£æ)
æŒ‡æ ‡æ”¶é›†å™¨
  â†“ (å­˜å‚¨)
æ—¶åºæ•°æ®åº“ (InfluxDB/Prometheus)
  â†“ (å¯è§†åŒ–)
ç›‘æ§ä»ªè¡¨æ¿ (Grafana)
  â†“ (å‘Šè­¦)
å‘Šè­¦ç³»ç»Ÿ (Email/Slack)
```

---

## ğŸ”§ å®æ–½æ­¥éª¤

### æ­¥éª¤ 1: å¢å¼º Trace æ—¥å¿—

**ç›®æ ‡**: ç¡®ä¿æ‰€æœ‰å…³é”®æŒ‡æ ‡éƒ½è¢«è®°å½•

```python
# app/services/trace_service.py

class TraceService:
    """å¢å¼ºçš„ Trace æœåŠ¡"""
    
    def log_llm_call(
        self,
        task_type: str,
        prompt: str,
        response: str,
        tokens: Dict[str, int],
        cost: float,
        latency: float,
        metadata: Dict = None
    ):
        """è®°å½• LLM è°ƒç”¨"""
        entry = {
            "type": "llm_call",
            "timestamp": datetime.now().isoformat(),
            "task_type": task_type,
            
            # Token ä¿¡æ¯
            "input_tokens": tokens["input"],
            "output_tokens": tokens["output"],
            "total_tokens": tokens["total"],
            
            # æˆæœ¬å’Œæ€§èƒ½
            "cost_usd": cost,
            "latency_ms": latency,
            
            # æ¨¡å‹ä¿¡æ¯
            "provider": metadata.get("provider"),
            "model": metadata.get("model"),
            
            # é…ç½®ä¿¡æ¯
            "config": {
                "include_reasoning": metadata.get("include_reasoning"),
                "max_reply_tokens": metadata.get("max_reply_tokens"),
                "use_compact_schemas": metadata.get("use_compact_schemas")
            },
            
            # å†…å®¹ï¼ˆå¯é€‰ï¼‰
            "prompt": prompt if self.log_content else None,
            "response": response if self.log_content else None
        }
        
        self._write_to_file(entry)
        self._send_to_metrics_collector(entry)  # æ–°å¢
```

---

### æ­¥éª¤ 2: è®¾ç½®æŒ‡æ ‡æ”¶é›†å™¨

**é€‰é¡¹ A: ä½¿ç”¨ Prometheus**

```python
# app/services/metrics_collector.py

from prometheus_client import Counter, Histogram, Gauge

# å®šä¹‰æŒ‡æ ‡
token_usage = Counter(
    'llm_tokens_total',
    'Total tokens used',
    ['task_type', 'token_type']  # input/output
)

llm_cost = Counter(
    'llm_cost_usd_total',
    'Total LLM cost in USD',
    ['task_type', 'model']
)

llm_latency = Histogram(
    'llm_latency_seconds',
    'LLM call latency',
    ['task_type', 'model']
)

llm_quality = Gauge(
    'llm_quality_score',
    'LLM response quality score',
    ['task_type', 'scenario']
)

class MetricsCollector:
    """Prometheus æŒ‡æ ‡æ”¶é›†å™¨"""
    
    def record_llm_call(self, data: Dict):
        """è®°å½• LLM è°ƒç”¨æŒ‡æ ‡"""
        # Token ä½¿ç”¨
        token_usage.labels(
            task_type=data["task_type"],
            token_type="input"
        ).inc(data["input_tokens"])
        
        token_usage.labels(
            task_type=data["task_type"],
            token_type="output"
        ).inc(data["output_tokens"])
        
        # æˆæœ¬
        llm_cost.labels(
            task_type=data["task_type"],
            model=data["model"]
        ).inc(data["cost_usd"])
        
        # å»¶è¿Ÿ
        llm_latency.labels(
            task_type=data["task_type"],
            model=data["model"]
        ).observe(data["latency_ms"] / 1000)  # è½¬æ¢ä¸ºç§’
```

**é€‰é¡¹ B: ä½¿ç”¨ InfluxDB**

```python
# app/services/metrics_collector.py

from influxdb_client import InfluxDBClient, Point
from influxdb_client.client.write_api import SYNCHRONOUS

class InfluxMetricsCollector:
    """InfluxDB æŒ‡æ ‡æ”¶é›†å™¨"""
    
    def __init__(self, url: str, token: str, org: str, bucket: str):
        self.client = InfluxDBClient(url=url, token=token, org=org)
        self.write_api = self.client.write_api(write_options=SYNCHRONOUS)
        self.bucket = bucket
        self.org = org
    
    def record_llm_call(self, data: Dict):
        """è®°å½• LLM è°ƒç”¨æŒ‡æ ‡"""
        point = Point("llm_call") \
            .tag("task_type", data["task_type"]) \
            .tag("model", data["model"]) \
            .tag("provider", data["provider"]) \
            .field("input_tokens", data["input_tokens"]) \
            .field("output_tokens", data["output_tokens"]) \
            .field("total_tokens", data["total_tokens"]) \
            .field("cost_usd", data["cost_usd"]) \
            .field("latency_ms", data["latency_ms"]) \
            .time(datetime.now())
        
        self.write_api.write(bucket=self.bucket, org=self.org, record=point)
```

---

### æ­¥éª¤ 3: é…ç½® Grafana ä»ªè¡¨æ¿

**ä»ªè¡¨æ¿é…ç½® (JSON)**:

```json
{
  "dashboard": {
    "title": "LLM Token Optimization Dashboard",
    "panels": [
      {
        "title": "Token Usage Over Time",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(llm_tokens_total[5m])",
            "legendFormat": "{{task_type}} - {{token_type}}"
          }
        ]
      },
      {
        "title": "Cost Per Hour",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(llm_cost_usd_total[1h])",
            "legendFormat": "{{task_type}}"
          }
        ]
      },
      {
        "title": "Average Latency",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, llm_latency_seconds)",
            "legendFormat": "P95 - {{task_type}}"
          }
        ]
      },
      {
        "title": "Token Reduction",
        "type": "stat",
        "targets": [
          {
            "expr": "(baseline_tokens - optimized_tokens) / baseline_tokens * 100"
          }
        ]
      }
    ]
  }
}
```

**å…³é”®é¢æ¿**:

1. **Token ä½¿ç”¨è¶‹åŠ¿**
   - æ—¶é—´åºåˆ—å›¾
   - æŒ‰ä»»åŠ¡ç±»å‹åˆ†ç»„
   - æ˜¾ç¤ºè¾“å…¥/è¾“å‡º token

2. **æˆæœ¬ç›‘æ§**
   - æ¯å°æ—¶æˆæœ¬
   - æ¯æ—¥æˆæœ¬ç´¯è®¡
   - æˆæœ¬é¢„æµ‹

3. **æ€§èƒ½æŒ‡æ ‡**
   - P50/P95/P99 å»¶è¿Ÿ
   - é”™è¯¯ç‡
   - è¯·æ±‚æˆåŠŸç‡

4. **ä¼˜åŒ–æ•ˆæœ**
   - Token å‡å°‘ç™¾åˆ†æ¯”
   - æˆæœ¬èŠ‚çœé‡‘é¢
   - è´¨é‡å¯¹æ¯”

---

### æ­¥éª¤ 4: è®¾ç½®å‘Šè­¦è§„åˆ™

**Prometheus å‘Šè­¦è§„åˆ™**:

```yaml
# prometheus_alerts.yml

groups:
  - name: llm_optimization_alerts
    interval: 1m
    rules:
      # Token ä½¿ç”¨è¿‡é«˜
      - alert: HighTokenUsage
        expr: rate(llm_tokens_total[5m]) > 10000
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High token usage detected"
          description: "Token usage is {{ $value }} tokens/sec"
      
      # æˆæœ¬è¿‡é«˜
      - alert: HighCost
        expr: rate(llm_cost_usd_total[1h]) > 10
        for: 10m
        labels:
          severity: critical
        annotations:
          summary: "High LLM cost detected"
          description: "Cost is ${{ $value }}/hour"
      
      # å»¶è¿Ÿè¿‡é«˜
      - alert: HighLatency
        expr: histogram_quantile(0.95, llm_latency_seconds) > 5
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High latency detected"
          description: "P95 latency is {{ $value }}s"
      
      # è´¨é‡ä¸‹é™
      - alert: QualityDegradation
        expr: llm_quality_score < 0.8
        for: 10m
        labels:
          severity: critical
        annotations:
          summary: "Quality degradation detected"
          description: "Quality score is {{ $value }}"
```

**å‘Šè­¦é€šçŸ¥é…ç½®**:

```yaml
# alertmanager.yml

route:
  receiver: 'team-email'
  group_by: ['alertname', 'severity']
  group_wait: 30s
  group_interval: 5m
  repeat_interval: 4h

receivers:
  - name: 'team-email'
    email_configs:
      - to: 'team@example.com'
        from: 'alerts@example.com'
        smarthost: 'smtp.gmail.com:587'
        auth_username: 'alerts@example.com'
        auth_password: 'password'
  
  - name: 'slack'
    slack_configs:
      - api_url: 'https://hooks.slack.com/services/xxx'
        channel: '#llm-alerts'
        title: 'LLM Optimization Alert'
        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
```

---

### æ­¥éª¤ 5: æ—¥å¿—åˆ†æè„šæœ¬

**æ¯æ—¥æŠ¥å‘Šç”Ÿæˆ**:

```python
# scripts/generate_daily_report.py

import asyncio
from datetime import datetime, timedelta
from scripts.analyze_trace import load_trace_file, extract_llm_calls

async def generate_daily_report():
    """ç”Ÿæˆæ¯æ—¥æŠ¥å‘Š"""
    
    # åŠ è½½ä»Šå¤©çš„ trace æ—¥å¿—
    today = datetime.now().date()
    trace_file = f"logs/trace_{today}.jsonl"
    
    entries = load_trace_file(trace_file)
    llm_calls = extract_llm_calls(entries)
    
    # è®¡ç®—ç»Ÿè®¡æ•°æ®
    total_tokens = sum(call["total_tokens"] for call in llm_calls)
    total_cost = sum(call["cost_usd"] for call in llm_calls)
    avg_latency = sum(call["latency_ms"] for call in llm_calls) / len(llm_calls)
    
    # æŒ‰ä»»åŠ¡ç±»å‹åˆ†ç»„
    by_task = {}
    for call in llm_calls:
        task = call["task_type"]
        if task not in by_task:
            by_task[task] = {
                "count": 0,
                "tokens": 0,
                "cost": 0
            }
        by_task[task]["count"] += 1
        by_task[task]["tokens"] += call["total_tokens"]
        by_task[task]["cost"] += call["cost_usd"]
    
    # ç”ŸæˆæŠ¥å‘Š
    report = f"""
# LLM Token ä¼˜åŒ– - æ¯æ—¥æŠ¥å‘Š

**æ—¥æœŸ**: {today}

## æ€»ä½“ç»Ÿè®¡

- æ€»è¯·æ±‚æ•°: {len(llm_calls):,}
- æ€» Token: {total_tokens:,}
- æ€»æˆæœ¬: ${total_cost:.2f}
- å¹³å‡å»¶è¿Ÿ: {avg_latency:.0f}ms

## æŒ‰ä»»åŠ¡ç±»å‹ç»Ÿè®¡

| ä»»åŠ¡ç±»å‹ | è¯·æ±‚æ•° | Token | æˆæœ¬ |
|---------|--------|-------|------|
"""
    
    for task, stats in by_task.items():
        report += f"| {task} | {stats['count']:,} | {stats['tokens']:,} | ${stats['cost']:.2f} |\n"
    
    # ä¿å­˜æŠ¥å‘Š
    report_file = f"reports/daily_report_{today}.md"
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
    
    # å‘é€é‚®ä»¶ï¼ˆå¯é€‰ï¼‰
    await send_email_report(report)

if __name__ == "__main__":
    asyncio.run(generate_daily_report())
```

**å®šæ—¶ä»»åŠ¡é…ç½® (cron)**:

```bash
# æ¯å¤©å‡Œæ™¨ 1 ç‚¹ç”ŸæˆæŠ¥å‘Š
0 1 * * * cd /path/to/project && python scripts/generate_daily_report.py

# æ¯å°æ—¶æ£€æŸ¥å‘Šè­¦
0 * * * * cd /path/to/project && python scripts/check_alerts.py
```

---

## ğŸ“ˆ ç›‘æ§æŒ‡æ ‡è¯¦è§£

### Token æŒ‡æ ‡

```python
# å…³é”® Token æŒ‡æ ‡
metrics = {
    # ä½¿ç”¨é‡
    "total_tokens": "æ€» token æ•°",
    "input_tokens": "è¾“å…¥ token æ•°",
    "output_tokens": "è¾“å‡º token æ•°",
    
    # æ•ˆç‡
    "tokens_per_request": "æ¯è¯·æ±‚å¹³å‡ token",
    "token_reduction_rate": "Token å‡å°‘ç‡",
    
    # åˆ†å¸ƒ
    "tokens_by_task": "æŒ‰ä»»åŠ¡ç±»å‹çš„ token åˆ†å¸ƒ",
    "tokens_by_model": "æŒ‰æ¨¡å‹çš„ token åˆ†å¸ƒ"
}
```

### æˆæœ¬æŒ‡æ ‡

```python
# å…³é”®æˆæœ¬æŒ‡æ ‡
metrics = {
    # æ€»æˆæœ¬
    "total_cost": "æ€»æˆæœ¬ (USD)",
    "cost_per_request": "æ¯è¯·æ±‚æˆæœ¬",
    "cost_per_user": "æ¯ç”¨æˆ·æˆæœ¬",
    
    # è¶‹åŠ¿
    "daily_cost": "æ¯æ—¥æˆæœ¬",
    "monthly_cost": "æ¯æœˆæˆæœ¬",
    "cost_trend": "æˆæœ¬è¶‹åŠ¿",
    
    # èŠ‚çœ
    "cost_savings": "æˆæœ¬èŠ‚çœ",
    "savings_rate": "èŠ‚çœç‡"
}
```

### è´¨é‡æŒ‡æ ‡

```python
# å…³é”®è´¨é‡æŒ‡æ ‡
metrics = {
    # è¯„åˆ†
    "quality_score": "è´¨é‡è¯„åˆ† (0-1)",
    "relevance_score": "ç›¸å…³æ€§è¯„åˆ†",
    "intimacy_check_pass_rate": "äº²å¯†åº¦æ£€æŸ¥é€šè¿‡ç‡",
    
    # ç”¨æˆ·åé¦ˆ
    "user_satisfaction": "ç”¨æˆ·æ»¡æ„åº¦",
    "complaint_rate": "æŠ•è¯‰ç‡"
}
```

### æ€§èƒ½æŒ‡æ ‡

```python
# å…³é”®æ€§èƒ½æŒ‡æ ‡
metrics = {
    # å»¶è¿Ÿ
    "avg_latency": "å¹³å‡å»¶è¿Ÿ",
    "p95_latency": "P95 å»¶è¿Ÿ",
    "p99_latency": "P99 å»¶è¿Ÿ",
    
    # å¯é æ€§
    "success_rate": "æˆåŠŸç‡",
    "error_rate": "é”™è¯¯ç‡",
    "timeout_rate": "è¶…æ—¶ç‡"
}
```

---

## ğŸ¯ ç›‘æ§æœ€ä½³å®è·µ

### 1. åˆ†å±‚ç›‘æ§

```
åº”ç”¨å±‚ç›‘æ§
â”œâ”€â”€ Token ä½¿ç”¨
â”œâ”€â”€ æˆæœ¬
â””â”€â”€ è´¨é‡

åŸºç¡€è®¾æ–½ç›‘æ§
â”œâ”€â”€ CPU/å†…å­˜
â”œâ”€â”€ ç½‘ç»œ
â””â”€â”€ ç£ç›˜

ä¸šåŠ¡ç›‘æ§
â”œâ”€â”€ ç”¨æˆ·æ´»è·ƒåº¦
â”œâ”€â”€ è½¬åŒ–ç‡
â””â”€â”€ ç•™å­˜ç‡
```

### 2. å‘Šè­¦ç­–ç•¥

**å‘Šè­¦çº§åˆ«**:
- **Critical**: ç«‹å³å¤„ç†ï¼ˆè´¨é‡ä¸‹é™ã€æˆæœ¬å¤±æ§ï¼‰
- **Warning**: éœ€è¦å…³æ³¨ï¼ˆToken ä½¿ç”¨åé«˜ï¼‰
- **Info**: ä»…è®°å½•ï¼ˆé…ç½®å˜æ›´ï¼‰

**å‘Šè­¦é™å™ª**:
- è®¾ç½®åˆç†çš„é˜ˆå€¼
- ä½¿ç”¨å‘Šè­¦åˆ†ç»„
- é¿å…å‘Šè­¦é£æš´

### 3. æ•°æ®ä¿ç•™

```python
# æ•°æ®ä¿ç•™ç­–ç•¥
retention_policy = {
    "raw_logs": "7 å¤©",      # åŸå§‹æ—¥å¿—
    "hourly_metrics": "30 å¤©",  # å°æ—¶çº§æŒ‡æ ‡
    "daily_metrics": "1 å¹´",    # æ—¥çº§æŒ‡æ ‡
    "monthly_metrics": "æ°¸ä¹…"   # æœˆçº§æŒ‡æ ‡
}
```

---

## âœ… éªŒè¯æ¸…å•

### ç›‘æ§ç³»ç»Ÿ
- [ ] Trace æ—¥å¿—æ­£å¸¸è®°å½•
- [ ] æŒ‡æ ‡æ”¶é›†å™¨è¿è¡Œæ­£å¸¸
- [ ] æ—¶åºæ•°æ®åº“è¿æ¥æ­£å¸¸
- [ ] Grafana ä»ªè¡¨æ¿å¯è®¿é—®

### å‘Šè­¦ç³»ç»Ÿ
- [ ] å‘Šè­¦è§„åˆ™é…ç½®å®Œæˆ
- [ ] å‘Šè­¦é€šçŸ¥æ¸ é“æµ‹è¯•é€šè¿‡
- [ ] å‘Šè­¦é™å™ªè§„åˆ™ç”Ÿæ•ˆ
- [ ] å€¼ç­äººå‘˜å·²åŸ¹è®­

### æŠ¥å‘Šç³»ç»Ÿ
- [ ] æ¯æ—¥æŠ¥å‘Šè‡ªåŠ¨ç”Ÿæˆ
- [ ] æŠ¥å‘Šå†…å®¹å‡†ç¡®å®Œæ•´
- [ ] æŠ¥å‘Šå‘é€æ­£å¸¸
- [ ] æŠ¥å‘Šå­˜æ¡£æ­£å¸¸

---

## ğŸ“š å‚è€ƒèµ„æ–™

- Prometheus æ–‡æ¡£: https://prometheus.io/docs/
- Grafana æ–‡æ¡£: https://grafana.com/docs/
- InfluxDB æ–‡æ¡£: https://docs.influxdata.com/
- Phase 3 å®ŒæˆæŠ¥å‘Š: `PHASE3_COMPLETION_REPORT.md`

---

**åˆ›å»ºæ—¥æœŸ**: 2026-01-22  
**æœ€åæ›´æ–°**: 2026-01-22  
**ç‰ˆæœ¬**: 1.0
