# Phase 5: æ™ºèƒ½è·¯ç”± (Prompt Router)

**ç›®æ ‡**: æ ¹æ®åœºæ™¯è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜æ¨¡å‹ï¼Œå‡å°‘ 40-60% æˆæœ¬  
**é¢„è®¡æ—¶é—´**: 1 å‘¨  
**ä¼˜å…ˆçº§**: â­â­ ä¸­

---

## ğŸ“‹ æ¦‚è¿°

ä¸åŒçš„åœºæ™¯å¯¹æ¨¡å‹èƒ½åŠ›çš„è¦æ±‚ä¸åŒã€‚Phase 5 é€šè¿‡æ™ºèƒ½è·¯ç”±ï¼Œæ ¹æ®å¯¹è¯åœºæ™¯ã€äº²å¯†åº¦ã€ç¨³å®šæ€§ç­‰å› ç´ ï¼Œè‡ªåŠ¨é€‰æ‹©æœ€åˆé€‚çš„æ¨¡å‹å’Œé…ç½®ï¼Œåœ¨ä¿è¯è´¨é‡çš„åŒæ—¶æœ€å¤§åŒ–æˆæœ¬æ•ˆç›Šã€‚

---

## ğŸ¯ ç›®æ ‡

### ä¸»è¦ç›®æ ‡
1. **æˆæœ¬ä¼˜åŒ–**
   - ç›®æ ‡: 40-60% æˆæœ¬å‡å°‘
   - ä¿æŒè´¨é‡ä¸å˜
   - æ™ºèƒ½æ¨¡å‹é€‰æ‹©

2. **è´¨é‡ä¿è¯**
   - å…³é”®åœºæ™¯ä½¿ç”¨é«˜è´¨é‡æ¨¡å‹
   - ç®€å•åœºæ™¯ä½¿ç”¨ç»æµæ¨¡å‹
   - åŠ¨æ€è°ƒæ•´ç­–ç•¥

3. **çµæ´»é…ç½®**
   - å¯é…ç½®çš„è·¯ç”±è§„åˆ™
   - A/B æµ‹è¯•æ”¯æŒ
   - å®æ—¶è°ƒæ•´èƒ½åŠ›

---

## ğŸ—ï¸ æ¶æ„è®¾è®¡

### è·¯ç”±å†³ç­–æµç¨‹

```
LLM è°ƒç”¨è¯·æ±‚
    â†“
æå–è·¯ç”±ä¸Šä¸‹æ–‡
    â†“
è·¯ç”±å™¨åˆ†æ
    â†“
é€‰æ‹©æ¨¡å‹/é…ç½®
    â†“
æ‰§è¡Œ LLM è°ƒç”¨
    â†“
è®°å½•å†³ç­–å’Œç»“æœ
```

### è·¯ç”±ä¸Šä¸‹æ–‡

```python
@dataclass
class RoutingContext:
    """è·¯ç”±å†³ç­–æ‰€éœ€çš„ä¸Šä¸‹æ–‡ä¿¡æ¯"""
    
    # åœºæ™¯ä¿¡æ¯
    scenario: str  # SAFE, BALANCED, RISKY, etc.
    intimacy_level: int  # 0-100
    current_intimacy_level: int  # 0-100
    
    # ç¨³å®šæ€§
    relationship_stability: str  # stable, unstable, critical
    
    # ä»»åŠ¡ç±»å‹
    task_type: str  # scene, strategy, generation
    
    # è´¨é‡è¦æ±‚
    quality_tier: str  # cheap, normal, premium
    
    # ç”¨æˆ·ä¿¡æ¯
    user_id: str
    is_vip: bool  # VIP ç”¨æˆ·å¯èƒ½éœ€è¦æ›´é«˜è´¨é‡
```

---

## ğŸ›ï¸ è·¯ç”±è§„åˆ™è®¾è®¡

### è·¯ç”±è¡¨

```python
ROUTING_TABLE = {
    # åœºæ™¯åˆ†æ - ä½¿ç”¨å¿«é€Ÿæ¨¡å‹
    "scene_analysis": {
        "default": {
            "provider": "dashscope",
            "model": "qwen-turbo",  # å¿«é€Ÿä¸”ä¾¿å®œ
            "max_tokens": 200,
            "temperature": 0.3
        }
    },
    
    # ç­–ç•¥è§„åˆ’ - æ ¹æ®åœºæ™¯é€‰æ‹©
    "strategy_planning": {
        "SAFE": {
            "provider": "dashscope",
            "model": "qwen-turbo",
            "max_tokens": 150,
            "temperature": 0.5
        },
        "BALANCED": {
            "provider": "dashscope",
            "model": "qwen-plus",
            "max_tokens": 200,
            "temperature": 0.6
        },
        "RISKY": {
            "provider": "dashscope",
            "model": "qwen-max",  # é«˜é£é™©åœºæ™¯éœ€è¦æ›´å¥½çš„æ¨¡å‹
            "max_tokens": 250,
            "temperature": 0.7
        },
        "RECOVERY": {
            "provider": "dashscope",
            "model": "qwen-max",  # ä¿®å¤æœŸéœ€è¦è°¨æ…
            "max_tokens": 250,
            "temperature": 0.5
        }
    },
    
    # å›å¤ç”Ÿæˆ - æ ¹æ®äº²å¯†åº¦å’Œç¨³å®šæ€§é€‰æ‹©
    "reply_generation": {
        # ä½äº²å¯†åº¦ + ç¨³å®š = ç»æµæ¨¡å‹
        "low_intimacy_stable": {
            "provider": "dashscope",
            "model": "qwen-turbo",
            "max_tokens": 100,
            "temperature": 0.7
        },
        
        # ä¸­ç­‰äº²å¯†åº¦ = æ ‡å‡†æ¨¡å‹
        "medium_intimacy": {
            "provider": "dashscope",
            "model": "qwen-plus",
            "max_tokens": 150,
            "temperature": 0.7
        },
        
        # é«˜äº²å¯†åº¦ = é«˜è´¨é‡æ¨¡å‹
        "high_intimacy": {
            "provider": "dashscope",
            "model": "qwen-max",
            "max_tokens": 200,
            "temperature": 0.8
        },
        
        # ä¸ç¨³å®šå…³ç³» = é«˜è´¨é‡æ¨¡å‹ï¼ˆéœ€è¦è°¨æ…ï¼‰
        "unstable": {
            "provider": "dashscope",
            "model": "qwen-max",
            "max_tokens": 200,
            "temperature": 0.6
        },
        
        # VIP ç”¨æˆ· = æœ€é«˜è´¨é‡
        "vip": {
            "provider": "dashscope",
            "model": "qwen-max",
            "max_tokens": 250,
            "temperature": 0.8
        }
    }
}
```

### è·¯ç”±é€»è¾‘

```python
class PromptRouter:
    """æ™ºèƒ½ Prompt è·¯ç”±å™¨"""
    
    def __init__(self, routing_table: Dict = None):
        self.routing_table = routing_table or ROUTING_TABLE
        self.decision_log = []  # è®°å½•è·¯ç”±å†³ç­–
    
    def route(self, context: RoutingContext) -> RoutingDecision:
        """æ ¹æ®ä¸Šä¸‹æ–‡åšå‡ºè·¯ç”±å†³ç­–
        
        Args:
            context: è·¯ç”±ä¸Šä¸‹æ–‡
        
        Returns:
            è·¯ç”±å†³ç­–ï¼ˆæ¨¡å‹ã€é…ç½®ç­‰ï¼‰
        """
        task_type = context.task_type
        
        # åœºæ™¯åˆ†æ - æ€»æ˜¯ä½¿ç”¨å¿«é€Ÿæ¨¡å‹
        if task_type == "scene_analysis":
            config = self.routing_table["scene_analysis"]["default"]
        
        # ç­–ç•¥è§„åˆ’ - æ ¹æ®åœºæ™¯é€‰æ‹©
        elif task_type == "strategy_planning":
            scenario = context.scenario
            config = self.routing_table["strategy_planning"].get(
                scenario,
                self.routing_table["strategy_planning"]["BALANCED"]
            )
        
        # å›å¤ç”Ÿæˆ - å¤æ‚çš„è·¯ç”±é€»è¾‘
        elif task_type == "reply_generation":
            config = self._route_reply_generation(context)
        
        else:
            # é»˜è®¤é…ç½®
            config = {
                "provider": "dashscope",
                "model": "qwen-plus",
                "max_tokens": 200,
                "temperature": 0.7
            }
        
        # åˆ›å»ºè·¯ç”±å†³ç­–
        decision = RoutingDecision(
            provider=config["provider"],
            model=config["model"],
            max_tokens=config["max_tokens"],
            temperature=config["temperature"],
            reasoning=self._explain_decision(context, config)
        )
        
        # è®°å½•å†³ç­–
        self._log_decision(context, decision)
        
        return decision
    
    def _route_reply_generation(self, context: RoutingContext) -> Dict:
        """å›å¤ç”Ÿæˆçš„è·¯ç”±é€»è¾‘"""
        
        # VIP ç”¨æˆ· - æœ€é«˜è´¨é‡
        if context.is_vip:
            return self.routing_table["reply_generation"]["vip"]
        
        # ä¸ç¨³å®šå…³ç³» - é«˜è´¨é‡æ¨¡å‹
        if context.relationship_stability == "unstable":
            return self.routing_table["reply_generation"]["unstable"]
        
        # æ ¹æ®äº²å¯†åº¦é€‰æ‹©
        intimacy = context.intimacy_level
        
        if intimacy >= 70:
            # é«˜äº²å¯†åº¦
            return self.routing_table["reply_generation"]["high_intimacy"]
        elif intimacy >= 40:
            # ä¸­ç­‰äº²å¯†åº¦
            return self.routing_table["reply_generation"]["medium_intimacy"]
        else:
            # ä½äº²å¯†åº¦ + ç¨³å®š
            if context.relationship_stability == "stable":
                return self.routing_table["reply_generation"]["low_intimacy_stable"]
            else:
                return self.routing_table["reply_generation"]["medium_intimacy"]
    
    def _explain_decision(self, context: RoutingContext, config: Dict) -> str:
        """è§£é‡Šè·¯ç”±å†³ç­–"""
        return f"Task: {context.task_type}, Scenario: {context.scenario}, " \
               f"Intimacy: {context.intimacy_level}, Model: {config['model']}"
    
    def _log_decision(self, context: RoutingContext, decision: RoutingDecision):
        """è®°å½•è·¯ç”±å†³ç­–"""
        self.decision_log.append({
            "timestamp": datetime.now(),
            "context": context,
            "decision": decision
        })
```

---

## ğŸ”— é›†æˆåˆ° LLM Adapter

### æ›´æ–° LLMAdapter

```python
class LLMAdapterImpl(BaseLLMAdapter):
    def __init__(
        self,
        router: PromptRouter = None,  # æ–°å¢
        trace_service: TraceService = None
    ):
        self.router = router  # æ–°å¢
        self.trace_service = trace_service
    
    async def call(self, llm_call: LLMCall) -> LLMResult:
        """æ‰§è¡Œ LLM è°ƒç”¨ï¼ˆå¸¦è·¯ç”±ï¼‰"""
        
        # å¦‚æœæœ‰è·¯ç”±å™¨ï¼Œä½¿ç”¨è·¯ç”±å†³ç­–
        if self.router and llm_call.routing_context:
            decision = self.router.route(llm_call.routing_context)
            
            # è¦†ç›–åŸæœ‰é…ç½®
            llm_call.provider = decision.provider
            llm_call.model = decision.model
            llm_call.max_tokens = decision.max_tokens
            llm_call.temperature = decision.temperature
            
            # è®°å½•è·¯ç”±å†³ç­–
            if self.trace_service:
                self.trace_service.log_routing_decision(decision)
        
        # æ‰§è¡ŒåŸæœ‰çš„è°ƒç”¨é€»è¾‘
        return await self._execute_call(llm_call)
```

### æ›´æ–° LLMCall

```python
@dataclass
class LLMCall:
    """LLM è°ƒç”¨è¯·æ±‚"""
    
    task_type: str
    prompt: str
    quality: str = "normal"
    user_id: str = "system"
    
    # è·¯ç”±ç›¸å…³ï¼ˆæ–°å¢ï¼‰
    routing_context: Optional[RoutingContext] = None
    
    # å¯è¢«è·¯ç”±å™¨è¦†ç›–çš„å­—æ®µ
    provider: Optional[str] = None
    model: Optional[str] = None
    max_tokens: Optional[int] = None
    temperature: Optional[float] = None
```

---

## ğŸ“Š é¢„æœŸæ•ˆæœ

### æˆæœ¬å‡å°‘

| åœºæ™¯ | åŸå§‹æ¨¡å‹ | è·¯ç”±åæ¨¡å‹ | æˆæœ¬å‡å°‘ |
|------|---------|-----------|---------|
| åœºæ™¯åˆ†æ | qwen-plus | qwen-turbo | -60% |
| ç­–ç•¥è§„åˆ’ (SAFE) | qwen-plus | qwen-turbo | -60% |
| ç­–ç•¥è§„åˆ’ (RISKY) | qwen-plus | qwen-max | +50% |
| å›å¤ç”Ÿæˆ (ä½äº²å¯†åº¦) | qwen-plus | qwen-turbo | -60% |
| å›å¤ç”Ÿæˆ (é«˜äº²å¯†åº¦) | qwen-plus | qwen-max | +50% |

### æ•´ä½“å½±å“

å‡è®¾è¯·æ±‚åˆ†å¸ƒï¼š
- åœºæ™¯åˆ†æ: 100% ä½¿ç”¨ turbo (-60%)
- ç­–ç•¥è§„åˆ’: 60% SAFE/BALANCED (turbo), 40% RISKY/RECOVERY (max)
- å›å¤ç”Ÿæˆ: 50% ä½äº²å¯†åº¦ (turbo), 30% ä¸­ç­‰ (plus), 20% é«˜äº²å¯†åº¦ (max)

**åŠ æƒå¹³å‡æˆæœ¬å‡å°‘**: ~45%

---

## ğŸ§ª æµ‹è¯•ç­–ç•¥

### å•å…ƒæµ‹è¯•

```python
# tests/test_prompt_router.py

class TestPromptRouter:
    def test_scene_analysis_routing(self):
        """æµ‹è¯•åœºæ™¯åˆ†æè·¯ç”±"""
        context = RoutingContext(
            task_type="scene_analysis",
            scenario="BALANCED",
            intimacy_level=50
        )
        
        decision = router.route(context)
        
        assert decision.model == "qwen-turbo"
    
    def test_risky_scenario_routing(self):
        """æµ‹è¯•é«˜é£é™©åœºæ™¯è·¯ç”±"""
        context = RoutingContext(
            task_type="strategy_planning",
            scenario="RISKY",
            intimacy_level=60
        )
        
        decision = router.route(context)
        
        assert decision.model == "qwen-max"
    
    def test_vip_user_routing(self):
        """æµ‹è¯• VIP ç”¨æˆ·è·¯ç”±"""
        context = RoutingContext(
            task_type="reply_generation",
            scenario="BALANCED",
            intimacy_level=50,
            is_vip=True
        )
        
        decision = router.route(context)
        
        assert decision.model == "qwen-max"
```

### A/B æµ‹è¯•

```python
# å¯¹æ¯”è·¯ç”±å‰åçš„æ•ˆæœ
async def ab_test_routing():
    """A/B æµ‹è¯•è·¯ç”±æ•ˆæœ"""
    
    # A ç»„: ä¸ä½¿ç”¨è·¯ç”±
    group_a_cost = 0
    group_a_quality = []
    
    # B ç»„: ä½¿ç”¨è·¯ç”±
    group_b_cost = 0
    group_b_quality = []
    
    for request in test_requests:
        # A ç»„
        result_a = await llm_adapter_without_router.call(request)
        group_a_cost += result_a.cost
        group_a_quality.append(evaluate_quality(result_a))
        
        # B ç»„
        result_b = await llm_adapter_with_router.call(request)
        group_b_cost += result_b.cost
        group_b_quality.append(evaluate_quality(result_b))
    
    # å¯¹æ¯”ç»“æœ
    cost_reduction = (group_a_cost - group_b_cost) / group_a_cost
    quality_change = (mean(group_b_quality) - mean(group_a_quality)) / mean(group_a_quality)
    
    print(f"æˆæœ¬å‡å°‘: {cost_reduction:.1%}")
    print(f"è´¨é‡å˜åŒ–: {quality_change:.1%}")
```

---

## ğŸ“ˆ ç›‘æ§ä¸ä¼˜åŒ–

### è·¯ç”±å†³ç­–ç›‘æ§

```python
# è®°å½•æ¯ä¸ªè·¯ç”±å†³ç­–
{
    "timestamp": "2026-01-22T10:00:00Z",
    "task_type": "reply_generation",
    "context": {
        "scenario": "BALANCED",
        "intimacy_level": 65,
        "stability": "stable"
    },
    "decision": {
        "model": "qwen-plus",
        "reasoning": "Medium intimacy, stable relationship"
    },
    "result": {
        "cost": 0.005,
        "quality_score": 0.85,
        "latency_ms": 1200
    }
}
```

### è·¯ç”±æ•ˆæœåˆ†æ

```python
def analyze_routing_effectiveness():
    """åˆ†æè·¯ç”±æ•ˆæœ"""
    
    # æŒ‰æ¨¡å‹ç»Ÿè®¡
    model_stats = {}
    for decision in router.decision_log:
        model = decision.decision.model
        if model not in model_stats:
            model_stats[model] = {
                "count": 0,
                "total_cost": 0,
                "avg_quality": []
            }
        
        model_stats[model]["count"] += 1
        model_stats[model]["total_cost"] += decision.result.cost
        model_stats[model]["avg_quality"].append(decision.result.quality_score)
    
    # æ‰“å°ç»Ÿè®¡
    for model, stats in model_stats.items():
        print(f"\n{model}:")
        print(f"  ä½¿ç”¨æ¬¡æ•°: {stats['count']}")
        print(f"  æ€»æˆæœ¬: ${stats['total_cost']:.2f}")
        print(f"  å¹³å‡è´¨é‡: {mean(stats['avg_quality']):.2f}")
```

---

## âš™ï¸ é…ç½®ä¸è°ƒä¼˜

### åŠ¨æ€è°ƒæ•´è·¯ç”±è§„åˆ™

```python
# æ ¹æ®å®é™…æ•°æ®è°ƒæ•´è·¯ç”±è¡¨
def optimize_routing_table(performance_data):
    """æ ¹æ®æ€§èƒ½æ•°æ®ä¼˜åŒ–è·¯ç”±è¡¨"""
    
    # åˆ†æå“ªäº›åœºæ™¯å¯ä»¥ä½¿ç”¨æ›´ä¾¿å®œçš„æ¨¡å‹
    for scenario, data in performance_data.items():
        if data["quality_score"] > 0.90 and data["model"] == "qwen-max":
            # è´¨é‡è¿‡é«˜ï¼Œå¯ä»¥é™çº§
            print(f"å»ºè®® {scenario} é™çº§åˆ° qwen-plus")
        
        elif data["quality_score"] < 0.80 and data["model"] == "qwen-turbo":
            # è´¨é‡ä¸è¶³ï¼Œéœ€è¦å‡çº§
            print(f"å»ºè®® {scenario} å‡çº§åˆ° qwen-plus")
```

### A/B æµ‹è¯•æ¡†æ¶

```python
class RoutingABTest:
    """è·¯ç”± A/B æµ‹è¯•æ¡†æ¶"""
    
    def __init__(self, variant_a: Dict, variant_b: Dict):
        self.variant_a = variant_a  # è·¯ç”±è§„åˆ™ A
        self.variant_b = variant_b  # è·¯ç”±è§„åˆ™ B
        self.results = {"a": [], "b": []}
    
    async def run_test(self, requests: List[LLMCall]):
        """è¿è¡Œ A/B æµ‹è¯•"""
        for request in requests:
            # éšæœºåˆ†é…åˆ° A æˆ– B ç»„
            variant = "a" if random.random() < 0.5 else "b"
            
            # ä½¿ç”¨å¯¹åº”çš„è·¯ç”±è§„åˆ™
            routing_table = self.variant_a if variant == "a" else self.variant_b
            router = PromptRouter(routing_table)
            
            # æ‰§è¡Œè¯·æ±‚
            result = await execute_with_router(request, router)
            
            # è®°å½•ç»“æœ
            self.results[variant].append(result)
    
    def analyze_results(self):
        """åˆ†æ A/B æµ‹è¯•ç»“æœ"""
        # å¯¹æ¯”æˆæœ¬ã€è´¨é‡ã€å»¶è¿Ÿç­‰æŒ‡æ ‡
        pass
```

---

## ğŸ“… å®æ–½è®¡åˆ’

### Week 1: æ ¸å¿ƒå®ç°
- Day 1-2: åˆ›å»º `PromptRouter` ç±»
- Day 3-4: é›†æˆåˆ° `LLMAdapter`
- Day 5: æ›´æ–°æ‰€æœ‰æœåŠ¡ä»¥æä¾›è·¯ç”±ä¸Šä¸‹æ–‡

### Week 2: æµ‹è¯•ä¸ä¼˜åŒ–
- Day 1-2: å•å…ƒæµ‹è¯•
- Day 3-4: A/B æµ‹è¯•
- Day 5: è·¯ç”±è§„åˆ™è°ƒä¼˜

### Week 3: éƒ¨ç½²ä¸ç›‘æ§
- Day 1-2: é‡‘ä¸é›€éƒ¨ç½²
- Day 3-5: å…¨é‡éƒ¨ç½²
- Day 6-7: ç›‘æ§å’Œä¼˜åŒ–

---

## ğŸ¯ æˆåŠŸæ ‡å‡†

- âœ… æˆæœ¬å‡å°‘ â‰¥ 40%
- âœ… è´¨é‡æŒ‡æ ‡æ— ä¸‹é™
- âœ… è·¯ç”±å†³ç­–å»¶è¿Ÿ < 10ms
- âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡
- âœ… ç›‘æ§ç³»ç»Ÿæ­£å¸¸è¿è¡Œ

---

## âš ï¸ æ³¨æ„äº‹é¡¹

### è´¨é‡é£é™©
- **é—®é¢˜**: è¿‡åº¦ä¼˜åŒ–å¯èƒ½é™ä½è´¨é‡
- **ç¼“è§£**: è®¾ç½®è´¨é‡ä¸‹é™ï¼Œå®šæœŸè¯„ä¼°
- **ç›‘æ§**: å®æ—¶è´¨é‡ç›‘æ§ï¼Œè‡ªåŠ¨å‘Šè­¦

### å¤æ‚æ€§
- **é—®é¢˜**: è·¯ç”±é€»è¾‘å¯èƒ½å˜å¾—å¤æ‚
- **ç¼“è§£**: ä¿æŒè§„åˆ™ç®€å•æ˜äº†
- **æ–‡æ¡£**: è¯¦ç»†è®°å½•æ¯ä¸ªè·¯ç”±å†³ç­–

### æ¨¡å‹å¯ç”¨æ€§
- **é—®é¢˜**: æŸäº›æ¨¡å‹å¯èƒ½ä¸å¯ç”¨
- **ç¼“è§£**: å®ç°é™çº§ç­–ç•¥
- **ç›‘æ§**: ç›‘æ§æ¨¡å‹å¯ç”¨æ€§

---

## ğŸ“š å‚è€ƒèµ„æ–™

- Phase 3 å®ŒæˆæŠ¥å‘Š: `PHASE3_COMPLETION_REPORT.md`
- Phase 4 è®¾è®¡: `PHASE4_MEMORY_COMPRESSION.md`
- å®æ–½æ¸…å•: `how_to_reduce_token/IMPLEMENTATION_CHECKLIST.md`

---

**åˆ›å»ºæ—¥æœŸ**: 2026-01-22  
**æœ€åæ›´æ–°**: 2026-01-22  
**ç‰ˆæœ¬**: 1.0
