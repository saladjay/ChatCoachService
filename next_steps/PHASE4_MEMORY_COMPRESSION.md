# Phase 4: å†…å­˜å‹ç¼© (Memory Compression)

**ç›®æ ‡**: å‹ç¼©å¯¹è¯å†å²ï¼Œå‡å°‘ 70% å†å² token  
**é¢„è®¡æ—¶é—´**: 1 å‘¨  
**ä¼˜å…ˆçº§**: â­â­ ä¸­é«˜

---

## ğŸ“‹ æ¦‚è¿°

é•¿å¯¹è¯å†å²ä¼šæ¶ˆè€—å¤§é‡ tokenã€‚Phase 4 é€šè¿‡æ™ºèƒ½å‹ç¼©æŠ€æœ¯ï¼Œå°†å†å²å¯¹è¯è½¬æ¢ä¸ºç´§å‡‘çš„è®°å¿†è¡¨ç¤ºï¼ŒåŒæ—¶ä¿æŒå…³é”®ä¿¡æ¯ã€‚

---

## ğŸ¯ ç›®æ ‡

### ä¸»è¦ç›®æ ‡
1. **å‡å°‘å†å² token ä½¿ç”¨**
   - ç›®æ ‡: 70% å‡å°‘
   - ä¿æŒä¸Šä¸‹æ–‡è´¨é‡
   - ä¸ä¸¢å¤±å…³é”®ä¿¡æ¯

2. **æ™ºèƒ½ä¿¡æ¯æå–**
   - æå–å…³é”®è¯é¢˜
   - åˆ†ææƒ…æ„Ÿè¶‹åŠ¿
   - è¯†åˆ«é‡è¦äº‹ä»¶

3. **é«˜æ•ˆå­˜å‚¨**
   - ç´§å‡‘çš„å†…å­˜æ ¼å¼
   - å¿«é€Ÿæ£€ç´¢
   - æ˜“äºæ›´æ–°

---

## ğŸ—ï¸ æ¶æ„è®¾è®¡

### ç»„ä»¶ç»“æ„

```
ConversationMemoryService
â”œâ”€â”€ compress_history()          # å‹ç¼©å¯¹è¯å†å²
â”œâ”€â”€ extract_topics()            # æå–è¯é¢˜
â”œâ”€â”€ analyze_tone_trend()        # åˆ†ææƒ…æ„Ÿè¶‹åŠ¿
â”œâ”€â”€ analyze_style()             # åˆ†æå¯¹è¯é£æ ¼
â””â”€â”€ format_memory_for_prompt()  # æ ¼å¼åŒ–ä¸º prompt
```

### æ•°æ®æµ

```
åŸå§‹å¯¹è¯ (100 æ¡æ¶ˆæ¯, ~2000 tokens)
    â†“
æå–å…³é”®ä¿¡æ¯
    â†“
å‹ç¼©ä¸ºè®°å¿† (~600 tokens, -70%)
    â†“
æ ¼å¼åŒ–ä¸º prompt
    â†“
ä¸æœ€è¿‘æ¶ˆæ¯ç»“åˆ (10 æ¡, ~200 tokens)
    â†“
æ€»è®¡: ~800 tokens (vs åŸå§‹ 2000 tokens)
```

---

## ğŸ’¾ å†…å­˜æ ¼å¼è®¾è®¡

### å‹ç¼©è®°å¿†ç»“æ„

```python
@dataclass
class ConversationMemory:
    """å‹ç¼©çš„å¯¹è¯è®°å¿†"""
    
    # åŸºæœ¬ä¿¡æ¯
    conversation_id: str
    user_id: str
    target_id: str
    
    # æ—¶é—´èŒƒå›´
    start_time: datetime
    end_time: datetime
    message_count: int
    
    # å…³é”®è¯é¢˜ (æœ€å¤š 5 ä¸ª)
    topics: List[str]  # ["çº¦ä¼šè®¡åˆ’", "å·¥ä½œå‹åŠ›", "å…´è¶£çˆ±å¥½"]
    
    # æƒ…æ„Ÿè¶‹åŠ¿
    tone_trend: str  # "positive â†’ neutral â†’ positive"
    
    # å…³é”®äº‹ä»¶ (æœ€å¤š 3 ä¸ª)
    key_events: List[str]  # ["ç”¨æˆ·æåˆ°æ˜å¤©æœ‰çº¦ä¼š", "è®¨è®ºäº†å·¥ä½œé—®é¢˜"]
    
    # å¯¹è¯é£æ ¼
    user_style: str  # "casual, friendly, open"
    target_style: str  # "supportive, empathetic"
    
    # äº²å¯†åº¦å˜åŒ–
    intimacy_change: str  # "50 â†’ 65 (+15)"
    
    # å‹ç¼©ç»Ÿè®¡
    original_tokens: int  # 2000
    compressed_tokens: int  # 600
    compression_ratio: float  # 0.70
```

### Prompt æ ¼å¼

```
å¯¹è¯è®°å¿† (è¿‡å» 100 æ¡æ¶ˆæ¯):
- æ—¶é—´: 2026-01-15 è‡³ 2026-01-22
- è¯é¢˜: çº¦ä¼šè®¡åˆ’, å·¥ä½œå‹åŠ›, å…´è¶£çˆ±å¥½
- æƒ…æ„Ÿ: ç§¯æ â†’ ä¸­æ€§ â†’ ç§¯æ
- å…³é”®äº‹ä»¶:
  * ç”¨æˆ·æåˆ°æ˜å¤©æœ‰ç¬¬ä¸€æ¬¡çº¦ä¼šï¼Œæ„Ÿåˆ°ç´§å¼ 
  * è®¨è®ºäº†å¦‚ä½•å¤„ç†å·¥ä½œå‹åŠ›
  * åˆ†äº«äº†å…±åŒçš„å…´è¶£çˆ±å¥½
- é£æ ¼: ç”¨æˆ·è½»æ¾å‹å¥½ï¼Œå¯¹æ–¹æ”¯æŒå…±æƒ…
- äº²å¯†åº¦: 50 â†’ 65 (+15)

æœ€è¿‘å¯¹è¯ (æœ€å 10 æ¡æ¶ˆæ¯):
[å®Œæ•´çš„æœ€è¿‘æ¶ˆæ¯]
```

---

## ğŸ”§ å®ç°ç»†èŠ‚

### 1. è¯é¢˜æå–

```python
async def extract_topics(self, messages: List[Message]) -> List[str]:
    """æå–å¯¹è¯ä¸­çš„å…³é”®è¯é¢˜
    
    ä½¿ç”¨ LLM åˆ†æå¯¹è¯å†…å®¹ï¼Œæå–æœ€å¤š 5 ä¸ªå…³é”®è¯é¢˜ã€‚
    
    Args:
        messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
    
    Returns:
        è¯é¢˜åˆ—è¡¨ï¼ŒæŒ‰é‡è¦æ€§æ’åº
    """
    # æ„å»ºç´§å‡‘çš„ prompt
    conversation_text = self._format_messages_compact(messages)
    
    prompt = f"""åˆ†æä»¥ä¸‹å¯¹è¯ï¼Œæå– 3-5 ä¸ªå…³é”®è¯é¢˜ã€‚
æ¯ä¸ªè¯é¢˜ç”¨ 2-4 ä¸ªè¯æè¿°ã€‚

å¯¹è¯:
{conversation_text}

è¾“å‡ºæ ¼å¼ (JSON):
{{"topics": ["è¯é¢˜1", "è¯é¢˜2", "è¯é¢˜3"]}}
"""
    
    # è°ƒç”¨ LLM (ä½¿ç”¨ cheap æ¨¡å‹)
    result = await self.llm_adapter.call(LLMCall(
        task_type="topic_extraction",
        prompt=prompt,
        quality="cheap",
        max_tokens=100
    ))
    
    # è§£æç»“æœ
    data = json.loads(result.text)
    return data["topics"][:5]
```

### 2. æƒ…æ„Ÿè¶‹åŠ¿åˆ†æ

```python
async def analyze_tone_trend(self, messages: List[Message]) -> str:
    """åˆ†æå¯¹è¯çš„æƒ…æ„Ÿè¶‹åŠ¿
    
    å°†å¯¹è¯åˆ†ä¸º 3 æ®µï¼Œåˆ†ææ¯æ®µçš„æƒ…æ„Ÿï¼Œè¿”å›è¶‹åŠ¿æè¿°ã€‚
    
    Args:
        messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
    
    Returns:
        æƒ…æ„Ÿè¶‹åŠ¿æè¿°ï¼Œå¦‚ "positive â†’ neutral â†’ positive"
    """
    # å°†æ¶ˆæ¯åˆ†ä¸º 3 æ®µ
    segment_size = len(messages) // 3
    segments = [
        messages[:segment_size],
        messages[segment_size:segment_size*2],
        messages[segment_size*2:]
    ]
    
    tones = []
    for segment in segments:
        # åˆ†ææ¯æ®µçš„æƒ…æ„Ÿ
        tone = await self._analyze_segment_tone(segment)
        tones.append(tone)
    
    # æ ¼å¼åŒ–è¶‹åŠ¿
    return " â†’ ".join(tones)
```

### 3. å…³é”®äº‹ä»¶æå–

```python
async def extract_key_events(self, messages: List[Message]) -> List[str]:
    """æå–å¯¹è¯ä¸­çš„å…³é”®äº‹ä»¶
    
    è¯†åˆ«é‡è¦çš„è½¬æŠ˜ç‚¹ã€å†³å®šã€æˆ–é‡è¦ä¿¡æ¯ã€‚
    
    Args:
        messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
    
    Returns:
        å…³é”®äº‹ä»¶åˆ—è¡¨ (æœ€å¤š 3 ä¸ª)
    """
    conversation_text = self._format_messages_compact(messages)
    
    prompt = f"""è¯†åˆ«å¯¹è¯ä¸­çš„å…³é”®äº‹ä»¶æˆ–é‡è¦ä¿¡æ¯ã€‚
æ¯ä¸ªäº‹ä»¶ç”¨ä¸€å¥è¯æè¿° (10-15 è¯)ã€‚

å¯¹è¯:
{conversation_text}

è¾“å‡ºæ ¼å¼ (JSON):
{{"events": ["äº‹ä»¶1", "äº‹ä»¶2", "äº‹ä»¶3"]}}
"""
    
    result = await self.llm_adapter.call(LLMCall(
        task_type="event_extraction",
        prompt=prompt,
        quality="cheap",
        max_tokens=150
    ))
    
    data = json.loads(result.text)
    return data["events"][:3]
```

### 4. å‹ç¼©å†å²

```python
async def compress_history(
    self,
    conversation_id: str,
    messages: List[Message],
    keep_recent: int = 10
) -> ConversationMemory:
    """å‹ç¼©å¯¹è¯å†å²
    
    å°†é•¿å¯¹è¯å†å²å‹ç¼©ä¸ºç´§å‡‘çš„è®°å¿†è¡¨ç¤ºã€‚
    
    Args:
        conversation_id: å¯¹è¯ ID
        messages: å®Œæ•´çš„æ¶ˆæ¯åˆ—è¡¨
        keep_recent: ä¿ç•™æœ€è¿‘çš„æ¶ˆæ¯æ•°é‡
    
    Returns:
        å‹ç¼©çš„å¯¹è¯è®°å¿†
    """
    # åˆ†ç¦»å†å²å’Œæœ€è¿‘æ¶ˆæ¯
    if len(messages) <= keep_recent:
        # æ¶ˆæ¯å¤ªå°‘ï¼Œä¸éœ€è¦å‹ç¼©
        return None
    
    history_messages = messages[:-keep_recent]
    
    # å¹¶è¡Œæå–ä¿¡æ¯
    topics_task = self.extract_topics(history_messages)
    tone_task = self.analyze_tone_trend(history_messages)
    events_task = self.extract_key_events(history_messages)
    style_task = self.analyze_style(history_messages)
    
    topics, tone_trend, key_events, styles = await asyncio.gather(
        topics_task, tone_task, events_task, style_task
    )
    
    # è®¡ç®—äº²å¯†åº¦å˜åŒ–
    intimacy_change = self._calculate_intimacy_change(history_messages)
    
    # ä¼°ç®— token æ•°
    original_tokens = self._estimate_tokens(history_messages)
    compressed_tokens = self._estimate_compressed_tokens(
        topics, tone_trend, key_events, styles, intimacy_change
    )
    
    # åˆ›å»ºè®°å¿†å¯¹è±¡
    memory = ConversationMemory(
        conversation_id=conversation_id,
        user_id=history_messages[0].user_id,
        target_id=history_messages[0].target_id,
        start_time=history_messages[0].timestamp,
        end_time=history_messages[-1].timestamp,
        message_count=len(history_messages),
        topics=topics,
        tone_trend=tone_trend,
        key_events=key_events,
        user_style=styles["user"],
        target_style=styles["target"],
        intimacy_change=intimacy_change,
        original_tokens=original_tokens,
        compressed_tokens=compressed_tokens,
        compression_ratio=(original_tokens - compressed_tokens) / original_tokens
    )
    
    return memory
```

### 5. æ ¼å¼åŒ–ä¸º Prompt

```python
def format_memory_for_prompt(self, memory: ConversationMemory) -> str:
    """å°†å‹ç¼©è®°å¿†æ ¼å¼åŒ–ä¸º prompt
    
    Args:
        memory: å‹ç¼©çš„å¯¹è¯è®°å¿†
    
    Returns:
        æ ¼å¼åŒ–çš„ prompt æ–‡æœ¬
    """
    if memory is None:
        return ""
    
    lines = [
        f"å¯¹è¯è®°å¿† (è¿‡å» {memory.message_count} æ¡æ¶ˆæ¯):",
        f"- æ—¶é—´: {memory.start_time.strftime('%Y-%m-%d')} è‡³ {memory.end_time.strftime('%Y-%m-%d')}",
        f"- è¯é¢˜: {', '.join(memory.topics)}",
        f"- æƒ…æ„Ÿ: {memory.tone_trend}",
        "- å…³é”®äº‹ä»¶:"
    ]
    
    for event in memory.key_events:
        lines.append(f"  * {event}")
    
    lines.extend([
        f"- é£æ ¼: ç”¨æˆ·{memory.user_style}ï¼Œå¯¹æ–¹{memory.target_style}",
        f"- äº²å¯†åº¦: {memory.intimacy_change}",
        ""
    ])
    
    return "\n".join(lines)
```

---

## ğŸ”— é›†æˆåˆ°ç°æœ‰ç³»ç»Ÿ

### æ›´æ–° ContextBuilder

```python
class ContextBuilder(BaseContextBuilder):
    def __init__(
        self,
        llm_adapter: BaseLLMAdapter,
        memory_service: ConversationMemoryService,  # æ–°å¢
        use_compact_prompt: bool = True
    ):
        self.llm_adapter = llm_adapter
        self.memory_service = memory_service  # æ–°å¢
        self.use_compact_prompt = use_compact_prompt
    
    async def build_context(
        self,
        user_id: str,
        conversation: List[Message],
        **kwargs
    ) -> ContextResult:
        # å‹ç¼©å†å²
        memory = await self.memory_service.compress_history(
            conversation_id=kwargs.get("conversation_id"),
            messages=conversation,
            keep_recent=10
        )
        
        # è·å–æœ€è¿‘æ¶ˆæ¯
        recent_messages = conversation[-10:]
        
        # æ„å»º prompt (ä½¿ç”¨è®°å¿† + æœ€è¿‘æ¶ˆæ¯)
        if memory:
            memory_text = self.memory_service.format_memory_for_prompt(memory)
            recent_text = self._format_recent_messages(recent_messages)
            full_context = f"{memory_text}\næœ€è¿‘å¯¹è¯:\n{recent_text}"
        else:
            full_context = self._format_recent_messages(recent_messages)
        
        # ç»§ç»­åŸæœ‰çš„ä¸Šä¸‹æ–‡æ„å»ºé€»è¾‘
        # ...
```

---

## ğŸ“Š é¢„æœŸæ•ˆæœ

### Token å‡å°‘

| åœºæ™¯ | åŸå§‹ Token | å‹ç¼©å Token | å‡å°‘ |
|------|-----------|-------------|------|
| çŸ­å¯¹è¯ (< 10 æ¡) | 200 | 200 | 0% |
| ä¸­ç­‰å¯¹è¯ (10-50 æ¡) | 1,000 | 400 | 60% |
| é•¿å¯¹è¯ (50-100 æ¡) | 2,000 | 600 | 70% |
| è¶…é•¿å¯¹è¯ (> 100 æ¡) | 4,000 | 800 | 80% |

### æˆæœ¬å½±å“

å‡è®¾æ¯å¤© 10,000 ä¸ªè¯·æ±‚ï¼Œå¹³å‡å¯¹è¯é•¿åº¦ 50 æ¡ï¼š
- åŸå§‹æˆæœ¬: 10,000 Ã— $0.010 = $100/å¤©
- å‹ç¼©åæˆæœ¬: 10,000 Ã— $0.004 = $40/å¤©
- **èŠ‚çœ: $60/å¤© = $1,800/æœˆ = $21,900/å¹´**

---

## ğŸ§ª æµ‹è¯•ç­–ç•¥

### å•å…ƒæµ‹è¯•

```python
# tests/test_conversation_memory.py

class TestConversationMemory:
    def test_topic_extraction(self):
        """æµ‹è¯•è¯é¢˜æå–"""
        # æµ‹è¯•èƒ½å¦æ­£ç¡®æå–è¯é¢˜
        
    def test_tone_analysis(self):
        """æµ‹è¯•æƒ…æ„Ÿåˆ†æ"""
        # æµ‹è¯•èƒ½å¦æ­£ç¡®åˆ†ææƒ…æ„Ÿè¶‹åŠ¿
        
    def test_compression_ratio(self):
        """æµ‹è¯•å‹ç¼©æ¯”ä¾‹"""
        # éªŒè¯å‹ç¼©æ¯”ä¾‹è¾¾åˆ° 70%
        
    def test_information_preservation(self):
        """æµ‹è¯•ä¿¡æ¯ä¿ç•™"""
        # éªŒè¯å…³é”®ä¿¡æ¯æ²¡æœ‰ä¸¢å¤±
```

### é›†æˆæµ‹è¯•

```python
# tests/integration/test_memory_compression.py

async def test_long_conversation_compression():
    """æµ‹è¯•é•¿å¯¹è¯å‹ç¼©"""
    # åˆ›å»º 100 æ¡æ¶ˆæ¯çš„å¯¹è¯
    messages = create_test_messages(100)
    
    # å‹ç¼©
    memory = await memory_service.compress_history(
        conversation_id="test",
        messages=messages
    )
    
    # éªŒè¯
    assert memory.compression_ratio >= 0.70
    assert len(memory.topics) <= 5
    assert len(memory.key_events) <= 3
```

---

## âš ï¸ æ³¨æ„äº‹é¡¹

### ä¿¡æ¯ä¸¢å¤±é£é™©
- **é—®é¢˜**: å‹ç¼©å¯èƒ½ä¸¢å¤±é‡è¦ç»†èŠ‚
- **ç¼“è§£**: ä¿ç•™æœ€è¿‘ 10 æ¡å®Œæ•´æ¶ˆæ¯
- **éªŒè¯**: äººå·¥è¯„ä¼°å‹ç¼©è´¨é‡

### LLM è°ƒç”¨æˆæœ¬
- **é—®é¢˜**: å‹ç¼©éœ€è¦é¢å¤–çš„ LLM è°ƒç”¨
- **ç¼“è§£**: ä½¿ç”¨ cheap æ¨¡å‹ï¼Œæ‰¹é‡å¤„ç†
- **ä¼˜åŒ–**: ç¼“å­˜å‹ç¼©ç»“æœ

### å»¶è¿Ÿå½±å“
- **é—®é¢˜**: å‹ç¼©å¢åŠ å“åº”æ—¶é—´
- **ç¼“è§£**: å¼‚æ­¥å‹ç¼©ï¼Œåå°å¤„ç†
- **ä¼˜åŒ–**: å¢é‡æ›´æ–°è®°å¿†

---

## ğŸ“… å®æ–½è®¡åˆ’

### Week 1: æ ¸å¿ƒå®ç°
- Day 1-2: åˆ›å»º `ConversationMemoryService`
- Day 3-4: å®ç°å‹ç¼©ç®—æ³•
- Day 5: é›†æˆåˆ° `ContextBuilder`

### Week 2: æµ‹è¯•ä¸ä¼˜åŒ–
- Day 1-2: å•å…ƒæµ‹è¯•
- Day 3-4: é›†æˆæµ‹è¯•
- Day 5: æ€§èƒ½ä¼˜åŒ–

### Week 3: éªŒè¯ä¸éƒ¨ç½²
- Day 1-2: æœ¬åœ°éªŒè¯
- Day 3-5: é‡‘ä¸é›€éƒ¨ç½²
- Day 6-7: å…¨é‡éƒ¨ç½²

---

## ğŸ¯ æˆåŠŸæ ‡å‡†

- âœ… å†å² token å‡å°‘ â‰¥ 70%
- âœ… å…³é”®ä¿¡æ¯ä¿ç•™ç‡ â‰¥ 95%
- âœ… å‹ç¼©å»¶è¿Ÿ < 500ms
- âœ… å›å¤è´¨é‡æ— ä¸‹é™
- âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡

---

## ğŸ“š å‚è€ƒèµ„æ–™

- Phase 3 å®ŒæˆæŠ¥å‘Š: `PHASE3_COMPLETION_REPORT.md`
- å®æ–½æ¸…å•: `how_to_reduce_token/IMPLEMENTATION_CHECKLIST.md`
- Token ä¼˜åŒ–åˆ†æ: `TOKEN_OPTIMIZATION_ANALYSIS.md`

---

**åˆ›å»ºæ—¥æœŸ**: 2026-01-22  
**æœ€åæ›´æ–°**: 2026-01-22  
**ç‰ˆæœ¬**: 1.0
