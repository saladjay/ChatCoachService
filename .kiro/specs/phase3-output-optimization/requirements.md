# Requirements Document: Phase 3 Output Optimization

## Introduction

Phase 3 focuses on reducing output token usage by 40-60% through controlling what information is included in LLM responses. This phase implements reasoning control, configuration management, and length constraints to minimize unnecessary output while maintaining quality.

## Glossary

- **Reasoning**: The explanation or thought process behind a generated reply
- **Output Token**: Tokens generated by the LLM in its response
- **Length Constraint**: Maximum token limit for generated content
- **Quality Tier**: Service level (cheap/normal/premium) determining output verbosity
- **Compact Schema**: Abbreviated output format using short field names and codes

## Requirements

### Requirement 1: Reasoning Control

**User Story:** As a system operator, I want to control whether reasoning is included in LLM outputs, so that I can reduce token usage when reasoning is not needed.

#### Acceptance Criteria

1. WHEN `include_reasoning` is set to False, THE System SHALL exclude reasoning fields from output schemas
2. WHEN `include_reasoning` is set to True, THE System SHALL include reasoning fields in output schemas
3. THE System SHALL support reasoning control for all LLM output types (scene analysis, strategy planning, reply generation)
4. WHEN reasoning is excluded, THE System SHALL maintain all other output fields
5. THE System SHALL default to `include_reasoning=False` for production environments

### Requirement 2: Configuration Management

**User Story:** As a developer, I want centralized configuration for output optimization, so that I can easily adjust settings without code changes.

#### Acceptance Criteria

1. THE System SHALL provide a `PromptConfig` class in `app/core/config.py`
2. THE `PromptConfig` SHALL include `include_reasoning` field (boolean)
3. THE `PromptConfig` SHALL include `max_reply_tokens` field (integer)
4. THE `PromptConfig` SHALL include `use_compact_schemas` field (boolean)
5. THE System SHALL load configuration from environment variables
6. THE System SHALL provide default values for all configuration fields
7. WHEN environment variables are not set, THE System SHALL use default values

### Requirement 3: Length Constraints

**User Story:** As a cost-conscious operator, I want to limit the length of generated replies, so that I can control output token costs.

#### Acceptance Criteria

1. THE System SHALL define `REPLY_LENGTH_CONSTRAINTS` mapping quality tiers to token limits
2. WHEN generating replies, THE System SHALL include length constraints in prompts
3. THE `LLMCall` schema SHALL include a `max_tokens` field
4. THE LLM Adapter SHALL enforce `max_tokens` limits when calling LLM APIs
5. THE System SHALL set default `max_tokens` based on quality tier:
   - cheap: 50 tokens
   - normal: 100 tokens
   - premium: 200 tokens
6. WHEN `max_tokens` is specified, THE System SHALL pass it to the LLM API
7. THE System SHALL handle cases where LLM output exceeds `max_tokens`

### Requirement 4: Prompt Assembly Integration

**User Story:** As a system component, I want prompt assembly to respect output optimization settings, so that prompts guide LLMs to produce optimized outputs.

#### Acceptance Criteria

1. WHEN `include_reasoning` is False, THE PromptAssembler SHALL modify output schema instructions to exclude reasoning
2. WHEN length constraints are set, THE PromptAssembler SHALL include length guidance in prompts
3. THE PromptAssembler SHALL use compact schemas when `use_compact_schemas` is True
4. THE PromptAssembler SHALL maintain backward compatibility with existing prompts
5. WHEN optimization settings change, THE PromptAssembler SHALL adapt prompts accordingly

### Requirement 5: Quality Preservation

**User Story:** As a quality assurance engineer, I want output optimization to maintain reply quality, so that user experience is not degraded.

#### Acceptance Criteria

1. WHEN reasoning is excluded, THE System SHALL maintain reply relevance and appropriateness
2. WHEN length constraints are applied, THE System SHALL prioritize essential information
3. THE System SHALL not truncate replies mid-sentence
4. THE System SHALL maintain intimacy level appropriateness
5. THE System SHALL preserve strategy alignment in generated replies

### Requirement 6: Testing and Validation

**User Story:** As a developer, I want comprehensive tests for output optimization, so that I can verify correctness and measure token savings.

#### Acceptance Criteria

1. THE System SHALL provide unit tests for reasoning control
2. THE System SHALL provide unit tests for length constraints
3. THE System SHALL provide integration tests measuring token reduction
4. THE System SHALL validate that quality is maintained
5. THE System SHALL measure actual token savings (target: 40-60%)
6. THE System SHALL document test results and token reduction metrics

### Requirement 7: Environment Configuration

**User Story:** As a deployment engineer, I want environment-based configuration, so that I can adjust settings per environment without code changes.

#### Acceptance Criteria

1. THE System SHALL support `PROMPT_INCLUDE_REASONING` environment variable
2. THE System SHALL support `PROMPT_MAX_REPLY_TOKENS` environment variable
3. THE System SHALL support `PROMPT_USE_COMPACT_SCHEMAS` environment variable
4. THE `.env.example` file SHALL document all output optimization variables
5. THE System SHALL validate environment variable values
6. WHEN invalid values are provided, THE System SHALL use defaults and log warnings

## Non-Functional Requirements

### Performance

1. Output optimization SHALL NOT increase latency by more than 50ms
2. Configuration loading SHALL occur at startup, not per-request
3. Prompt assembly SHALL remain efficient with optimization enabled

### Compatibility

1. Output optimization SHALL be backward compatible with existing code
2. Disabling optimization SHALL restore original behavior
3. Configuration changes SHALL NOT require code redeployment

### Maintainability

1. Configuration SHALL be centralized in `app/core/config.py`
2. Optimization logic SHALL be modular and testable
3. Code SHALL include clear documentation of optimization behavior

## Success Criteria

1. **Token Reduction**: 40-60% reduction in output tokens
2. **Quality Maintenance**: No degradation in reply quality metrics
3. **Test Coverage**: 100% coverage of optimization features
4. **Configuration**: All settings configurable via environment variables
5. **Documentation**: Complete documentation of optimization features

## Dependencies

- Phase 1: Schema Compression (completed)
- Phase 2: Prompt Layering (completed)
- Existing prompt assembly infrastructure
- LLM adapter with API call capabilities

## Risks and Mitigations

### Risk 1: Quality Degradation
**Mitigation**: Comprehensive testing, gradual rollout, quality metrics monitoring

### Risk 2: Over-Aggressive Truncation
**Mitigation**: Smart truncation logic, sentence boundary detection, minimum length guarantees

### Risk 3: Configuration Complexity
**Mitigation**: Sensible defaults, clear documentation, validation logic

## Timeline

- **Day 1-2**: Implement reasoning control
- **Day 2-3**: Add configuration management
- **Day 3-4**: Implement length constraints
- **Day 5**: Testing and validation

**Total Duration**: 5 days
